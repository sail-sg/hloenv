diff --git a/WORKSPACE b/WORKSPACE
deleted file mode 100644
index 1286ef9ac03..00000000000
--- a/WORKSPACE
+++ /dev/null
@@ -1,23 +0,0 @@
-workspace(name = "org_tensorflow")
-
-# Initialize the TensorFlow repository and all dependencies.
-#
-# The cascade of load() statements and tf_workspace?() calls works around the
-# restriction that load() statements need to be at the top of .bzl files.
-# E.g. we can not retrieve a new repository with http_archive and then load()
-# a macro from that repository in the same file.
-load("@//tensorflow:workspace3.bzl", "tf_workspace3")
-
-tf_workspace3()
-
-load("@//tensorflow:workspace2.bzl", "tf_workspace2")
-
-tf_workspace2()
-
-load("@//tensorflow:workspace1.bzl", "tf_workspace1")
-
-tf_workspace1()
-
-load("@//tensorflow:workspace0.bzl", "tf_workspace0")
-
-tf_workspace0()
diff --git a/passes.txt b/passes.txt
new file mode 100644
index 00000000000..b1fd72d56f1
--- /dev/null
+++ b/passes.txt
@@ -0,0 +1,62 @@
+algsimp
+all-gather-bcast-reorder
+all-reduce-folder
+all-reduce-reassociate
+all_to_all_decomposer
+batchnorm_expander
+bf16-normalization
+bitcast_dtypes_expander
+CallInliner
+comparison-expander
+conditional canonicalizer
+constant_folding
+convolution_4d_expander
+cse
+cublas-gemm-broadcast-folding-rewriter
+cublas-gemm-rewriter
+cublas-pad-for-gemms
+cudnn-fused-convolution-rewriter
+cudnn_pad_for_convolutions
+cudnn_vectorize_convolutions
+dce
+dot_decomposer
+dot-merger
+dynamic dimension simplifier
+dynamic-index-splitter
+dynamic_padder
+eigh_expander
+flatten-call-graph
+fusion
+fusion_merger
+gather_expander
+gemm-algorithm-picker
+gpu-conv-algorithm-picker
+gpu-conv-padding-legalization
+gpu-conv-rewriter
+gpu_scatter_expander
+gpusolver-rewriter
+layout-assignment
+logistic-expander
+multi_output_fusion
+operand_upcaster
+qr_expander
+real_imag_expander
+reduce-scatter-creator
+reduction-degenerate-dim-remover
+reduction-dimension-grouper
+reduction-layout-normalizer
+reshape-mover
+result_caster
+rng-bit-generator-expander
+rng-expander
+scatter_expander
+simplify-conditional
+simplify-sorts
+simplify-while-loops
+stable-sort-expander
+transpose-folding
+tuple-simplifier
+variadic-op-splitter
+while-loop-constant-sinking
+while-loop-trip-count-annotator
+zero_sized_hlo_elimination
diff --git a/tensorflow/compiler/xla/BUILD b/tensorflow/compiler/xla/BUILD
index 364111160f2..35c628af43b 100644
--- a/tensorflow/compiler/xla/BUILD
+++ b/tensorflow/compiler/xla/BUILD
@@ -156,7 +156,7 @@ cc_library(
     name = "types",
     hdrs = ["types.h"],
     compatible_with = get_compatible_with_portable(),
-    visibility = [":friends"],
+    visibility = ["//visibility:public"],
     deps = [
         "//third_party/eigen3",
     ],
@@ -549,7 +549,7 @@ cc_library(
     name = "literal_comparison",
     srcs = ["literal_comparison.cc"],
     hdrs = ["literal_comparison.h"],
-    visibility = [":friends"],
+    visibility = ["//visibility:public"],
     deps = [
         ":error_spec",
         ":literal",
diff --git a/tensorflow/compiler/xla/client/local_client.h b/tensorflow/compiler/xla/client/local_client.h
index a0ff1ced9c1..2d649c164cf 100644
--- a/tensorflow/compiler/xla/client/local_client.h
+++ b/tensorflow/compiler/xla/client/local_client.h
@@ -141,6 +141,7 @@ class LocalClient : public Client {
   //
   // The given ExecutableBuildOptions overrides any values from XLA_FLAGS
   // environment variable.
+  // MARK: compilation call chain
   StatusOr<std::vector<std::unique_ptr<LocalExecutable>>> Compile(
       const XlaComputation& computation,
       const absl::Span<const Shape* const> argument_layouts,
diff --git a/tensorflow/compiler/xla/pjrt/BUILD b/tensorflow/compiler/xla/pjrt/BUILD
index 97417b3ad93..0705d4c144a 100644
--- a/tensorflow/compiler/xla/pjrt/BUILD
+++ b/tensorflow/compiler/xla/pjrt/BUILD
@@ -136,7 +136,7 @@ cc_library(
     name = "pjrt_client",
     srcs = ["pjrt_client.cc"],
     hdrs = ["pjrt_client.h"],
-    visibility = ["//tensorflow/compiler/xla:friends"],
+    visibility = ["//visibility:public"],
     deps = [
         ":pjrt_future",
         "//tensorflow/compiler/xla:literal",
@@ -309,7 +309,7 @@ cc_library(
     srcs = ["cpu_device.cc"],
     hdrs = ["cpu_device.h"],
     compatible_with = [],
-    visibility = [":friends"],
+    visibility = ["//visibility:public"],
     deps = [
         ":pjrt_stream_executor_client",
         "//tensorflow/compiler/xla:statusor",
@@ -325,7 +325,7 @@ cc_library(
     srcs = ["gpu_device.cc"],
     hdrs = ["gpu_device.h"],
     defines = if_cuda(["GOOGLE_CUDA=1"]) + if_rocm(["TENSORFLOW_USE_ROCM=1"]),
-    visibility = [":friends"],
+    visibility = ["//visibility:public"],
     deps = [
         ":pjrt_stream_executor_client",
         "@com_google_absl//absl/base:core_headers",
diff --git a/tensorflow/compiler/xla/pjrt/pjrt_client.h b/tensorflow/compiler/xla/pjrt/pjrt_client.h
index d2274a1fd7b..a2cc93dd832 100644
--- a/tensorflow/compiler/xla/pjrt/pjrt_client.h
+++ b/tensorflow/compiler/xla/pjrt/pjrt_client.h
@@ -857,6 +857,9 @@ struct CompiledMemoryStats {
 // when passed to the execution.
 class PjRtExecutable {
  public:
+  mutable uint64_t compute_time_ns = 0;
+  mutable uint64_t async_exec_time_ns = 0;
+
   virtual ~PjRtExecutable() = default;
 
   virtual PjRtClient* client() const = 0;
diff --git a/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc b/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc
index b0e31850d43..df74a1f4755 100644
--- a/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc
+++ b/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc
@@ -1982,6 +1982,7 @@ StatusOr<ScopedShapedBuffer> PjRtStreamExecutorExecutable::EnqueueExecution(
           argument_handles, *device_buffers, events));
 
   ExecutableRunOptions run_options;
+  ExecutionProfile exec_profile;
   run_options.set_stream(device_state->compute_stream());
   run_options.set_host_to_device_stream(device_state->host_to_device_stream());
   run_options.set_allocator(client_->allocator());
@@ -1992,6 +1993,7 @@ StatusOr<ScopedShapedBuffer> PjRtStreamExecutorExecutable::EnqueueExecution(
   run_options.set_rng_seed(device_state->GetNewPrngSeed());
   run_options.set_gpu_executable_run_options(client_->gpu_run_options());
   run_options.set_launch_id(options.launch_id);
+  run_options.set_execution_profile(&exec_profile);
   if (run_options.launch_id() != 0) {
     VLOG(3) << "launch id for " << name() << ": " << run_options.launch_id();
   }
@@ -2008,9 +2010,12 @@ StatusOr<ScopedShapedBuffer> PjRtStreamExecutorExecutable::EnqueueExecution(
         device_state->compute_semaphore().ScopedAcquire(1));
   }
 
+  uint64_t start_time_ns = tensorflow::Env::Default()->NowNanos();
   StatusOr<ExecutionOutput> result_buffer_or_status =
-      executables_[executable_idx]->RunAsync(std::move(execution_inputs),
-                                             run_options);
+      executables_[executable_idx]->Run(std::move(execution_inputs),
+                                        run_options);
+  compute_time_ns = run_options.execution_profile()->compute_time_ns();
+  async_exec_time_ns = tensorflow::Env::Default()->NowNanos() - start_time_ns;
 
   VLOG(1) << "Replica " << replica << " partition " << partition
           << " completed; ok=" << result_buffer_or_status.ok();
@@ -2454,6 +2459,7 @@ PjRtStreamExecutorClient::GetExecutableExtras(CompileOptions* options) {
   return extras;
 }
 
+// MARK: compilation call chain
 StatusOr<std::unique_ptr<PjRtExecutable>> PjRtStreamExecutorClient::Compile(
     const XlaComputation& computation, CompileOptions options) {
   tensorflow::profiler::TraceMe traceme("PjRtStreamExecutorClient::Compile");
diff --git a/tensorflow/compiler/xla/python/BUILD b/tensorflow/compiler/xla/python/BUILD
index c62f94be19e..7e68f283983 100644
--- a/tensorflow/compiler/xla/python/BUILD
+++ b/tensorflow/compiler/xla/python/BUILD
@@ -147,6 +147,7 @@ cc_library(
         "@pybind11",
         "@pybind11_abseil//pybind11_abseil:absl_casters",
     ],
+    visibility = ["//visibility:public"],
 )
 
 cc_library(
diff --git a/tensorflow/compiler/xla/service/BUILD b/tensorflow/compiler/xla/service/BUILD
index bfd1014a694..4107ff5e203 100644
--- a/tensorflow/compiler/xla/service/BUILD
+++ b/tensorflow/compiler/xla/service/BUILD
@@ -551,6 +551,7 @@ cc_library(
         "hlo_module_metadata.h",
         "hlo_op_metadata.h",
         "hlo_opcode.h",
+        "hlo_reachability.h",
         "hlo_schedule.h",
         "hlo_sharding.h",
         "hlo_sharding_metadata.h",
@@ -588,6 +589,7 @@ cc_library(
         "@com_google_absl//absl/types:optional",
         "@com_google_absl//absl/types:span",
     ],
+    visibility = ["//visibility:public"],
 )
 
 cc_library(
@@ -1086,6 +1088,7 @@ cc_library(
     name = "service",
     srcs = ["service.cc"],
     hdrs = ["service.h"],
+    visibility = ["//visibility:public"],
     deps = [
         ":allocation_tracker",
         ":backend",
@@ -1981,6 +1984,17 @@ cc_library(
     ],
 )
 
+cc_library(
+    name = "dry_mode",
+    srcs = ["dry_mode.cc"],
+    hdrs = ["dry_mode.h"],
+    deps = [
+        ":hlo",
+        ":hlo_creation_utils",
+        ":hlo_pass",
+    ],
+)
+
 cc_library(
     name = "gather_expander",
     srcs = ["gather_expander.cc"],
@@ -4630,6 +4644,7 @@ cc_library(
     ],
     deps = [
         ":compilation_stats",
+        ":dry_mode",
         ":dump",
         ":hlo",
         ":hlo_graph_dumper",
@@ -4640,12 +4655,14 @@ cc_library(
         "//tensorflow/compiler/xla:types",
         "//tensorflow/compiler/xla:util",
         "//tensorflow/core:lib",
+        "//tensorflow/core/util:env_var",
         "@com_google_absl//absl/container:flat_hash_map",
         "@com_google_absl//absl/container:flat_hash_set",
         "@com_google_absl//absl/memory",
         "@com_google_absl//absl/strings",
         "@com_google_absl//absl/strings:str_format",
     ],
+    visibility = ["//visibility:public"],
 )
 
 tf_cc_test(
diff --git a/tensorflow/compiler/xla/service/bfloat16_normalization.h b/tensorflow/compiler/xla/service/bfloat16_normalization.h
index 3c4e7f641e4..870a1419d2a 100644
--- a/tensorflow/compiler/xla/service/bfloat16_normalization.h
+++ b/tensorflow/compiler/xla/service/bfloat16_normalization.h
@@ -32,6 +32,7 @@ class BFloat16Normalization : public HloModulePass {
 
   ~BFloat16Normalization() override = default;
   absl::string_view name() const override { return "bf16-normalization"; }
+  const BFloat16Support* bfloat16_support() { return bfloat16_support_; }
 
   // Run BF16 normalization on the given computation. Returns whether the
   // computation was changed.
diff --git a/tensorflow/compiler/xla/service/compiler.h b/tensorflow/compiler/xla/service/compiler.h
index 5ec03f07f87..2ee74ef0f3d 100644
--- a/tensorflow/compiler/xla/service/compiler.h
+++ b/tensorflow/compiler/xla/service/compiler.h
@@ -359,6 +359,21 @@ class Compiler {
   GetPlatformCompilers();
 };
 
+template <typename XlaCompiler>
+class Intercept : public std::exception {
+ public:
+  Intercept() {}
+  Intercept(XlaCompiler* c, std::unique_ptr<HloModule> m,
+            se::StreamExecutor* s, const Compiler::CompileOptions& o)
+      : compiler(c), module(std::move(m)), stream_exec(s), options(o) {}
+  Intercept(XlaCompiler* c, std::unique_ptr<HloModule> m)
+      : compiler(c), module(std::move(m)) {}
+  XlaCompiler* compiler;
+  std::unique_ptr<HloModule> module;
+  se::StreamExecutor* stream_exec;
+  Compiler::CompileOptions options;
+};
+
 }  // namespace xla
 
 #endif  // TENSORFLOW_COMPILER_XLA_SERVICE_COMPILER_H_
diff --git a/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc b/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc
index 64f44245a4b..700ee1afcde 100644
--- a/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc
+++ b/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc
@@ -777,6 +777,8 @@ Status CreateHloProfilingArtifacts(
 StatusOr<std::unique_ptr<HloModule>> CpuCompiler::RunHloPasses(
     std::unique_ptr<HloModule> module, se::StreamExecutor* /*stream_exec*/,
     const CompileOptions& /*options*/) {
+  throw Intercept<CpuCompiler>(this, std::move(module)); // This is where we handover control
+
   std::unique_ptr<llvm::TargetMachine> jit_target_machine =
       SimpleOrcJIT::InferTargetMachineForJIT(
           CompilerTargetOptions(module->config()),
diff --git a/tensorflow/compiler/xla/service/dfs_hlo_visitor.h b/tensorflow/compiler/xla/service/dfs_hlo_visitor.h
index 6849149c74b..2913ea780a3 100644
--- a/tensorflow/compiler/xla/service/dfs_hlo_visitor.h
+++ b/tensorflow/compiler/xla/service/dfs_hlo_visitor.h
@@ -291,6 +291,8 @@ class DfsHloVisitorBase {
   virtual Status HandleAddDependency(HloInstructionPtr add_dependency) = 0;
   virtual Status HandleAfterAll(HloInstructionPtr token) = 0;
 
+  virtual Status HandleAlternatives(HloInstructionPtr alternatives) = 0;
+
   // Invoked to inform the visitor that the traversal has completed, and that
   // the root was "root".
   virtual Status FinishVisit(HloInstructionPtr root) = 0;
diff --git a/tensorflow/compiler/xla/service/dfs_hlo_visitor_with_default.h b/tensorflow/compiler/xla/service/dfs_hlo_visitor_with_default.h
index 55e9d83e565..beec11f9685 100644
--- a/tensorflow/compiler/xla/service/dfs_hlo_visitor_with_default.h
+++ b/tensorflow/compiler/xla/service/dfs_hlo_visitor_with_default.h
@@ -274,7 +274,9 @@ class DfsHloVisitorWithDefaultBase
   Status HandleAddDependency(HloInstructionPtr add_dependency) override {
     return DefaultAction(add_dependency);
   }
-
+  Status HandleAlternatives(HloInstructionPtr alternatives) override {
+    return DefaultAction(alternatives);
+  }
   // Invoked to inform the visitor that the traversal has completed, and that
   // the root was "root".
   Status FinishVisit(HloInstructionPtr /*root*/) override {
diff --git a/tensorflow/compiler/xla/service/dry_mode.cc b/tensorflow/compiler/xla/service/dry_mode.cc
new file mode 100644
index 00000000000..2e713146296
--- /dev/null
+++ b/tensorflow/compiler/xla/service/dry_mode.cc
@@ -0,0 +1,50 @@
+#include "tensorflow/compiler/xla/service/dry_mode.h"
+
+#include <fstream>
+#include <random>
+#include <sstream>
+
+#include "tensorflow/compiler/xla/service/hlo_instructions.h"
+
+namespace xla {
+
+StatusOr<bool> DryModeOn::Run(HloModule* module) {
+  for (HloComputation* computation : module->MakeNonfusionComputations()) {
+    computation->set_dry(true);
+  }
+  return false;
+}
+
+StatusOr<bool> DryModeOff::Run(HloModule* module) {
+  bool changed = false;
+  for (HloComputation* computation : module->MakeNonfusionComputations()) {
+    computation->set_dry(false);
+  }
+  // std::cout << module->ToString() << std::endl;
+  for (auto* comp : module->MakeNonfusionComputations()) {
+    std::cout << comp->ToString() << std::endl;
+  }
+  std::random_device device;
+  std::mt19937 generator(device());
+  for (HloComputation* computation : module->MakeNonfusionComputations()) {
+    int num_replace = 0;
+    for (HloInstruction* inst : computation->instructions()) {
+      std::uniform_int_distribution<int> rand_selection(
+          0, inst->operand_count() - 1);
+      if (inst->opcode() == HloOpcode::kAlternatives) {
+        // Select a random index for alternatives
+        static_cast<HloAlternatives*>(inst)->Select(rand_selection(generator));
+        num_replace++;
+      }
+    }
+    if (num_replace > 0) {
+      changed = true;
+      LOG(ERROR) << num_replace << " alternatives replaced";
+    }
+    // Remove the residues
+    computation->Prune();
+  }
+  return changed;
+}
+
+}  // namespace xla
diff --git a/tensorflow/compiler/xla/service/dry_mode.h b/tensorflow/compiler/xla/service/dry_mode.h
new file mode 100644
index 00000000000..dce899c69c9
--- /dev/null
+++ b/tensorflow/compiler/xla/service/dry_mode.h
@@ -0,0 +1,40 @@
+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_COMPILER_XLA_SERVICE_DRYRUN_H_
+#define TENSORFLOW_COMPILER_XLA_SERVICE_DRYRUN_H_
+
+#include "tensorflow/compiler/xla/service/hlo_computation.h"
+#include "tensorflow/compiler/xla/service/hlo_pass_interface.h"
+
+namespace xla {
+
+class DryModeOn : public HloModulePass {
+ public:
+  absl::string_view name() const override { return "dry-mode-on"; }
+  explicit DryModeOn() {}
+  StatusOr<bool> Run(HloModule* module) override;
+};
+
+class DryModeOff : public HloModulePass {
+ public:
+  absl::string_view name() const override { return "dry-mode-off"; }
+  explicit DryModeOff() {}
+  StatusOr<bool> Run(HloModule* module) override;
+};
+
+}  // namespace xla
+
+#endif  // TENSORFLOW_COMPILER_XLA_SERVICE_DRYRUN_H_
diff --git a/tensorflow/compiler/xla/service/gpu/BUILD b/tensorflow/compiler/xla/service/gpu/BUILD
index 4280b3bf5cf..c4964f6d24a 100644
--- a/tensorflow/compiler/xla/service/gpu/BUILD
+++ b/tensorflow/compiler/xla/service/gpu/BUILD
@@ -1275,6 +1275,27 @@ tf_cc_test(
     ],
 )
 
+cc_library(
+    name = "general_fusion",
+    srcs = ["general_fusion.cc"],
+    hdrs = ["general_fusion.h"],
+    deps = [
+        ":gpu_fusible",
+        ":instruction_fusion",
+        ":ir_emission_utils",
+        "//tensorflow/compiler/xla:shape_util",
+        "//tensorflow/compiler/xla:util",
+        "//tensorflow/compiler/xla/service:hlo",
+        "//tensorflow/compiler/xla/service:hlo_cost_analysis",
+        "//tensorflow/compiler/xla/service:hlo_graph_dumper",
+        "//tensorflow/compiler/xla/service:hlo_pass",
+        "//tensorflow/compiler/xla/service/llvm_ir:fused_ir_emitter",
+        "//tensorflow/core:lib",
+        "@com_google_absl//absl/algorithm:container",
+        "@com_google_absl//absl/strings",
+    ],
+)
+
 cc_library(
     name = "gpu_conv_padding_legalization",
     srcs = ["gpu_conv_padding_legalization.cc"],
@@ -1507,6 +1528,7 @@ cc_library(
     name = "gpu_compiler",
     srcs = [
         "gpu_compiler.cc",
+        "gpu_compiler_ext.h",
     ],
     hdrs = [
         "gpu_compiler.h",
@@ -1522,6 +1544,7 @@ cc_library(
         ":fusion_merger",
         ":gemm_broadcast_folding_rewriter",
         ":gemm_rewriter",
+        ":general_fusion",
         ":gpu_constants",
         ":gpu_conv_algorithm_picker",
         ":gpu_conv_rewriter",
@@ -1660,6 +1683,7 @@ cc_library(
         ],
         "//conditions:default": [],
     }),
+    visibility = ["//visibility:public"],
 )
 
 cc_library(
diff --git a/tensorflow/compiler/xla/service/gpu/general_fusion.cc b/tensorflow/compiler/xla/service/gpu/general_fusion.cc
new file mode 100644
index 00000000000..0d1dba67b51
--- /dev/null
+++ b/tensorflow/compiler/xla/service/gpu/general_fusion.cc
@@ -0,0 +1,450 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/compiler/xla/service/gpu/general_fusion.h"
+
+#include <algorithm>
+#include <vector>
+
+#include "absl/algorithm/container.h"
+#include "absl/container/flat_hash_map.h"
+#include "absl/container/flat_hash_set.h"
+#include "absl/memory/memory.h"
+#include "absl/strings/str_join.h"
+#include "tensorflow/compiler/xla/debug_options_flags.h"
+#include "tensorflow/compiler/xla/map_util.h"
+#include "tensorflow/compiler/xla/service/fusion_queue.h"
+#include "tensorflow/compiler/xla/service/gpu/gpu_fusible.h"
+#include "tensorflow/compiler/xla/service/gpu/instruction_fusion.h"
+#include "tensorflow/compiler/xla/service/gpu/ir_emission_utils.h"
+#include "tensorflow/compiler/xla/service/hlo_cost_analysis.h"
+#include "tensorflow/compiler/xla/service/hlo_dataflow_analysis.h"
+#include "tensorflow/compiler/xla/service/hlo_graph_dumper.h"
+#include "tensorflow/compiler/xla/service/hlo_instruction.h"
+#include "tensorflow/compiler/xla/service/hlo_opcode.h"
+#include "tensorflow/compiler/xla/service/hlo_reachability.h"
+#include "tensorflow/compiler/xla/service/llvm_ir/fused_ir_emitter.h"
+#include "tensorflow/compiler/xla/service/pattern_matcher.h"
+#include "tensorflow/compiler/xla/shape_util.h"
+#include "tensorflow/compiler/xla/util.h"
+#include "tensorflow/core/lib/core/errors.h"
+#include "tensorflow/core/platform/logging.h"
+
+namespace xla {
+namespace gpu {
+
+bool IsParameter(const HloInstruction* inst) {
+  return inst->opcode() == HloOpcode::kParameter;
+}
+
+bool IsFusion(const HloInstruction* inst) {
+  return inst->opcode() == HloOpcode::kFusion;
+}
+
+bool IsGTE(const HloInstruction* inst) {
+  return inst->opcode() == HloOpcode::kGetTupleElement;
+}
+
+bool IsTuple(const HloInstruction* inst) {
+  return inst->opcode() == HloOpcode::kTuple;
+}
+
+bool IsRoot(const HloInstruction* inst) {
+  return inst->parent()->root_instruction() == inst;
+}
+
+void FuseTwo(HloInstruction* consumer, HloInstruction* producer) {
+  std::unordered_map<HloInstruction*, HloInstruction*> replacements;
+  std::vector<HloInstruction*> extra;
+
+  // Create the replacement map
+  // replace parameter with the real external input
+  for (int i = 0; i < producer->operand_count(); ++i) {
+    HloInstruction* operand = producer->mutable_operand(i);
+    if (IsFusion(producer)) {
+      HloComputation* p_comp = producer->fused_instructions_computation();
+      replacements[p_comp->parameter_instruction(i)] = operand;
+    }
+  }
+
+  for (int i = 0; i < consumer->operand_count(); ++i) {
+    HloInstruction* operand = consumer->mutable_operand(i);
+    HloInstruction* key;
+    HloInstruction* value;
+    if (IsFusion(consumer)) {
+      HloComputation* c_comp = consumer->fused_instructions_computation();
+      key = c_comp->parameter_instruction(i);
+    } else {
+      key = operand;
+    }
+    if (!IsGTE(operand)) {  // not a GTE
+      if (operand == producer) {
+        if (IsFusion(producer)) {
+          value = producer->fused_expression_root();
+        } else {
+          value = producer;
+        }
+      } else {
+        value = operand;
+      }
+    } else {  // operand is a GTE
+      if (operand->operand(0) == producer) {
+        if (IsFusion(producer)) {
+          // If producer is a fused computation, connect directly without GTE
+          value = producer->fused_expression_root()->mutable_operand(
+              operand->tuple_index());
+        } else {
+          // when this is an instruction that generates a tuple
+          value = operand;
+          // gte is useful only when the instruction provides a tuple output
+          // but is not fusion
+          extra.push_back(operand);
+        }
+      } else {
+        value = operand;
+      }
+    }
+    if (key != value) {
+      replacements[key] = value;
+    }
+  }
+
+  std::unordered_map<
+      HloInstruction*,
+      std::unordered_map<HloInstruction*, std::vector<HloInstruction*>>>
+      replace_users;
+  // new fusion root
+  std::vector<HloInstruction*> new_fused_root;
+  // indices of the instructions in the new_fusion_root that makes the root of
+  // the computation
+  std::vector<int> root_indices;
+
+  auto track_external_users =
+      [&](HloInstruction* inst, HloInstruction* to_replace, bool for_producer) {
+        std::vector<HloInstruction*> users;
+        users.reserve(to_replace->user_count());
+        if (for_producer) {
+          for (auto* u : to_replace->users()) {
+            if (u != consumer && u->dry()) {
+              users.push_back(u);
+              LOG(ERROR) << "inst: " << inst->name() << " user " << u->name();
+            }
+          }
+        }
+        // keep only external user
+        if (for_producer && !users.empty()) {
+          // Only add to the new root instruction tuple if the shape is 
+          // compatible. Since we need to ensure that the output shapes
+          // are consistent.
+          if (ShapesCompatibleForMultiOutputFusion(*inst, *consumer)) {
+            new_fused_root.push_back(inst);
+            replace_users[inst][to_replace] = users;
+          }
+        } else if (!for_producer) {
+          new_fused_root.push_back(inst);
+          std::vector<HloInstruction*> users;
+          std::copy_if(to_replace->users().begin(), to_replace->users().end(),
+                       std::back_inserter(users),
+                       [](HloInstruction* inst) { return inst->dry(); });
+          replace_users[inst][to_replace] = users;
+        }
+      };
+
+  // replacement for producer
+  auto track_uses = [&](HloInstruction* inst) {
+    bool is_consumer = (inst == consumer);
+    int num_root_inst = new_fused_root.size();
+    if (IsFusion(inst)) {
+      HloInstruction* root = inst->fused_expression_root();
+      if (IsTuple(root)) {
+        std::vector<bool> not_seen(root->mutable_operands().size(), true);
+        for (auto* gte : inst->users()) {
+          if (!gte->dry()) {
+            continue;
+          }
+          auto* key = root->mutable_operand(gte->tuple_index());
+          LOG(INFO) << key->name();
+          track_external_users(key, gte, !is_consumer);
+          not_seen.at(gte->tuple_index()) = false;
+        }
+
+        // Add back outputs that do not get used
+        for (int i = 0; i < not_seen.size(); i++) {
+          if (not_seen.at(i)) {
+            new_fused_root.push_back(root->mutable_operand(i));
+          }
+        }
+
+      } else {
+        LOG(INFO) << root->name();
+        track_external_users(root, inst, !is_consumer);
+      }
+    } else {
+      LOG(INFO) << inst->name();
+      track_external_users(inst, inst, !is_consumer);
+    }
+    if (is_consumer) {
+      if (IsRoot(inst)) {
+        root_indices.resize(new_fused_root.size() - num_root_inst);
+        std::iota(root_indices.begin(), root_indices.end(), num_root_inst);
+      }
+    }
+  };
+  track_uses(producer);
+  track_uses(consumer);
+  for (auto* gte : extra) {
+    track_external_users(gte, gte, true);
+  }
+  CHECK(!new_fused_root.empty());
+
+  std::vector<HloInstruction*> post_order;
+  std::unordered_map<HloInstruction*, HloInstruction*> old_to_new;
+  // put everything in topo order
+  if (IsFusion(producer)) {
+    post_order =
+        producer->fused_instructions_computation()->MakeInstructionPostOrder();
+  } else {
+    post_order.push_back(producer);
+  }
+  post_order.insert(post_order.end(), extra.begin(), extra.end());
+  if (IsFusion(consumer)) {
+    auto consumer_comp =
+        consumer->fused_instructions_computation()->MakeInstructionPostOrder();
+    post_order.insert(post_order.end(),
+                      std::make_move_iterator(consumer_comp.begin()),
+                      std::make_move_iterator(consumer_comp.end()));
+  } else {
+    post_order.push_back(consumer);
+  }
+
+  // now clone
+  HloComputation::Builder builder("fuse_two");
+  int param_index = 0;
+  std::unordered_map<HloInstruction*, HloInstruction*> extern_to_param;
+  std::vector<HloInstruction*> kfusion_operands;
+  for (auto* inst : post_order) {
+    // don't clone parameter & tuple
+    if (IsParameter(inst) || IsTuple(inst)) {
+      continue;
+    }
+    const auto& operands = inst->operands();
+    std::vector<HloInstruction*> new_operands;
+    for (auto* operand : operands) {
+      // First find the replacement
+      if (replacements.find(operand) != replacements.end()) {
+        operand = replacements[operand];
+      }
+
+      if (old_to_new.find(operand) != old_to_new.end()) {
+        // Then replace with the newly cloned instructions
+        operand = old_to_new[operand];
+      } else if (extern_to_param.find(operand) != extern_to_param.end()) {
+        // If not found, means it is external instruction,
+        // If a param is already associated with it, reuse the param
+        operand = extern_to_param[operand];
+      } else {
+        // Otherwise create a new param for it
+        auto* param = builder.AddInstruction(HloInstruction::CreateParameter(
+            param_index, operand->shape(),
+            "param" + std::to_string(param_index)));
+        extern_to_param[operand] = param;
+        kfusion_operands.push_back(operand);
+        operand = param;
+        param_index++;
+      }
+      new_operands.push_back(operand);
+    }
+    old_to_new[inst] = builder.AddInstruction(
+        inst->CloneWithNewOperands(inst->shape(), new_operands));
+    LOG(INFO) << "Cloning " << inst->name() << " to "
+              << old_to_new[inst]->name();
+  }
+
+  HloInstruction* root = nullptr;
+  std::vector<HloInstruction*> tuple_operands;
+  for (auto* inst : new_fused_root) {
+    tuple_operands.push_back(old_to_new[inst]);
+  }
+  if (tuple_operands.size() > 1) {
+    root = builder.AddInstruction(HloInstruction::CreateTuple(tuple_operands));
+  } else {
+    root = tuple_operands[0];
+  }
+  HloComputation* comp =
+      producer->parent()->parent()->AddEmbeddedComputation(builder.Build(root));
+  HloInstruction* fusion =
+      consumer->parent()->AddInstruction(HloInstruction::CreateFusion(
+          root->shape(), ChooseFusionKind(*producer, *consumer),
+          kfusion_operands, comp));
+
+  for (int i = 0; i < new_fused_root.size(); ++i) {
+    auto* inst = new_fused_root[i];
+    for (auto& kv : replace_users[inst]) {
+      auto* to_replace = kv.first;
+      auto& users = kv.second;
+      if (new_fused_root.size() > 1) {
+        HloInstruction* gte = consumer->parent()->AddInstruction(
+            HloInstruction::CreateGetTupleElement(fusion, i));
+        to_replace->ReplaceUsesWith(users, gte);
+      } else {
+        to_replace->ReplaceUsesWith(users, fusion);
+      }
+    }
+  }
+
+  if (IsRoot(consumer)) {
+    HloInstruction* new_root = nullptr;
+    if (root_indices.size() == new_fused_root.size()) {
+      new_root = fusion;
+    } else {
+      std::vector<HloInstruction*> operands;
+      for (int i : root_indices) {
+        HloInstruction* gte = consumer->parent()->AddInstruction(
+            HloInstruction::CreateGetTupleElement(fusion, i));
+        operands.push_back(gte);
+      }
+      if (operands.size() == 1) {
+        new_root = operands[0];
+      } else {
+        new_root = consumer->parent()->AddInstruction(
+            HloInstruction::CreateTuple(operands));
+      }
+    }
+    consumer->parent()->set_root_instruction(new_root);
+  }
+}
+
+bool GeneralFusion::HasCycle(HloInstruction* producer,
+                             HloInstruction* consumer) {
+  for (const HloInstruction* operand : consumer->operands()) {
+    if (operand == producer) {
+      continue;
+    }
+    else if (operand->opcode() == HloOpcode::kGetTupleElement) {
+      if (operand->operand(0) == producer) {
+        continue;
+      }
+    }
+    // For consumer's every other operand, check if reachable by producer.
+    // If so then creates a cycle.
+    if (reachability_->IsPresent(producer) &&
+        reachability_->IsPresent(operand) &&
+        reachability_->IsReachable(producer, operand)) {
+      return true;
+    }
+  }
+  return false;
+}
+
+bool GeneralFusion::ShouldFuseProducerIntoConsumer(HloInstruction* producer,
+                                                   HloInstruction* consumer) {
+  if (!producer->IsFusible()) {
+    return false;
+  }
+  if (HasCycle(producer, consumer)) {
+    return false;
+  }
+  if (producer->opcode() == HloOpcode::kCustomCall ||
+      consumer->opcode() == HloOpcode::kCustomCall) {
+    return false;
+  }
+  bool fusible = true;
+
+  int external_user_count = 0;
+  HloInstruction* actual_user;
+  // We need to check only for users that aren't parameters in the consumer.
+  for (auto* user : producer->users()) {
+    if (IsGTE(user)) {
+      for (auto* gte_user : user->users()) {
+        if (gte_user != consumer) {
+          external_user_count += 1;
+        }
+      }
+    }
+    else {
+      if (user != consumer) {
+        external_user_count += 1;
+      }
+    }
+  }
+
+  bool shapes_compatible = ShapesCompatibleForMultiOutputFusion(*consumer, 
+                                                                *producer);
+  // If shapes are not compatible, we don't fuse in the other consumer->user
+  // outputs as multiple outputs
+  // This is to ensure multi-output fusion has identical output shapes
+  if ((external_user_count > 0) && shapes_compatible) {
+    // MultiOutputFusion
+    fusible = IsProducerConsumerMultiOutputFusible(*producer, *consumer, 
+                                                   /*general_fusion*/ true);
+  } else {
+    FusionDecision decision = IsProducerConsumerFusible(*producer, *consumer,
+                                                   /*general_fusion*/ true);
+    fusible = decision.CanFuse();
+  }
+  return fusible;
+}
+
+bool GeneralFusion::DoGeneralFusion(HloComputation* comp) {
+  reachability_.reset();
+  reachability_ = HloReachabilityMap::Build(comp);
+  std::vector<HloInstruction*> post_order = comp->MakeInstructionPostOrder();
+  int count = 0;
+  while (!post_order.empty()) {
+    HloInstruction* inst = post_order.back();
+    post_order.pop_back();
+    if (IsGTE(inst) || IsTuple(inst) || !inst->IsFusible()) {
+      continue;
+    }
+    LOG(INFO) << "Considering consumer " << inst->name();
+    HloInstructionSet uniq_operands;
+    for (const auto* const_operand : inst->unique_operands()) {
+      auto* operand = const_cast<HloInstruction*>(const_operand);
+      if (IsGTE(operand)) {
+        LOG(INFO) << "Considering GTE producer " << operand->name();
+        // If GTE, trace back one more step
+        uniq_operands.insert(operand->mutable_operand(0));
+      } else {
+        LOG(INFO) << "Considering normal producer " << operand->name();
+        uniq_operands.insert(operand);
+      }
+    }
+    for (auto* operand : uniq_operands) {
+      LOG(INFO) << "Considering " << operand->name() << " and " << inst->name();
+      if (!ShouldFuseProducerIntoConsumer(/*producer=*/operand,
+                                          /*consumer=*/inst)) {
+        continue;
+      }
+      LOG(INFO) << "Fusing " << operand->name() << " into " << inst->name();
+      FuseTwo(/*consumer=*/inst, /*producer=*/operand);
+      count++;
+    }
+  }
+  return count > 0;
+}
+
+StatusOr<bool> GeneralFusion::Run(HloModule* module) {
+  bool changed = false;
+  for (auto* computation : module->MakeNonfusionComputations()) {
+    changed |= DoGeneralFusion(computation);
+  }
+  VLOG(1) << "After running GeneralFusion for module: " << module->name();
+  XLA_VLOG_LINES(3, module->ToString());
+  LOG(ERROR) << "Run Finish " << changed;
+  return changed;
+}
+
+}  // namespace gpu
+}  // namespace xla
diff --git a/tensorflow/compiler/xla/service/gpu/general_fusion.h b/tensorflow/compiler/xla/service/gpu/general_fusion.h
new file mode 100644
index 00000000000..d614fcac435
--- /dev/null
+++ b/tensorflow/compiler/xla/service/gpu/general_fusion.h
@@ -0,0 +1,54 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_COMPILER_XLA_SERVICE_GPU_GENERAL_FUSION_H_
+#define TENSORFLOW_COMPILER_XLA_SERVICE_GPU_GENERAL_FUSION_H_
+
+#include "tensorflow/compiler/xla/service/hlo_module.h"
+#include "tensorflow/compiler/xla/service/hlo_pass_interface.h"
+#include "tensorflow/compiler/xla/service/hlo_reachability.h"
+
+namespace xla {
+namespace gpu {
+
+// An HLO pass that does general fusion.
+
+class GeneralFusion : public HloModulePass {
+ public:
+  absl::string_view name() const override { return "general_fusion"; }
+
+  StatusOr<bool> Run(HloModule* module) override;
+
+  int fused_count() { return fused_count_; }
+
+ private:
+  bool FuseSiblings(std::vector<HloInstruction*>& siblings);
+  bool HasCycle(HloInstruction* producer, HloInstruction* consumer);
+  bool DoGeneralFusion(HloComputation* comp);
+  bool ShouldFuseProducerIntoConsumer(HloInstruction* producer, HloInstruction* consumer);
+
+  // Current computation the pass is dealing with.
+  HloComputation* computation_;
+
+  // Reachability map of current computation.
+  std::unique_ptr<HloReachabilityMap> reachability_;
+
+  int fused_count_;
+};
+
+}  // namespace gpu
+}  // namespace xla
+
+#endif  // TENSORFLOW_COMPILER_XLA_SERVICE_GPU_GENERAL_FUSION_H_
diff --git a/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc b/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc
index 7b40c9822c1..bbbf106c66a 100644
--- a/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc
+++ b/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc
@@ -86,6 +86,7 @@ limitations under the License.
 #include "tensorflow/compiler/xla/service/gpu/fusion_merger.h"
 #include "tensorflow/compiler/xla/service/gpu/gemm_broadcast_folding_rewriter.h"
 #include "tensorflow/compiler/xla/service/gpu/gemm_rewriter.h"
+#include "tensorflow/compiler/xla/service/gpu/general_fusion.h"
 #include "tensorflow/compiler/xla/service/gpu/gpu_constants.h"
 #include "tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.h"
 #include "tensorflow/compiler/xla/service/gpu/gpu_conv_rewriter.h"
@@ -185,63 +186,6 @@ namespace xla {
 namespace gpu {
 namespace {
 
-class GpuBfloat16Support : public BFloat16Support {
- public:
-  explicit GpuBfloat16Support(bool supports_matrix_multiplication,
-                              se::StreamExecutor* stream_exec)
-      : supports_matrix_multiplication_(supports_matrix_multiplication),
-        stream_exec_(stream_exec) {}
-
-  bool SupportsBF16Operand(const HloInstruction& hlo,
-                           int64_t operand_index) const override {
-    return BFloat16Support::SupportsBF16Operand(hlo, operand_index) ||
-           IsSupported(hlo);
-  }
-
-  // Returns whether the backend supports BF16 output for the HLO instruction.
-  bool SupportsBF16Output(const HloInstruction& hlo) const override {
-    return BFloat16Support::SupportsBF16Output(hlo) || IsSupported(hlo);
-  }
-
- private:
-  bool IsSupported(const HloInstruction& hlo) const {
-    switch (hlo.opcode()) {
-      case HloOpcode::kAllGather:
-      case HloOpcode::kAllReduce:
-      case HloOpcode::kAllReduceStart:
-      case HloOpcode::kAllReduceDone:
-      case HloOpcode::kReduceScatter:
-      case HloOpcode::kAllToAll:
-      case HloOpcode::kBitcast:
-      case HloOpcode::kCollectivePermute:
-        return true;
-      case HloOpcode::kConvolution:
-        return IsConvBF16Supported();
-      default:
-        return supports_matrix_multiplication_ &&
-               gpu::IsMatrixMultiplication(hlo);
-    }
-  }
-
-  bool IsConvBF16Supported() const {
-    if (se::dnn::DnnSupport* dnn = stream_exec_->AsDnn()) {
-      se::port::StatusOr<se::dnn::VersionInfo> cudnn_version =
-          dnn->GetVersion();
-      return cudnn_version.ok() &&
-             (cudnn_version->major_version() > 8 ||
-              (cudnn_version->major_version() == 8 &&
-               cudnn_version->minor_version() >= 2)) &&
-             stream_exec_->GetDeviceDescription()
-                 .cuda_compute_capability()
-                 .IsAtLeast(se::CudaComputeCapability::AMPERE);
-    }
-    return false;
-  }
-
-  bool supports_matrix_multiplication_;
-  se::StreamExecutor* stream_exec_;
-};
-
 int64_t GetSizeOfShape(const Shape& shape, int pointer_size) {
   if (shape.is_static() || shape.IsTuple()) {
     return ShapeUtil::ByteSizeOf(shape, pointer_size);
@@ -291,6 +235,7 @@ GpuCompiler::GpuCompiler(se::Platform::Id platform_id,
 Status GpuCompiler::OptimizeHloModule(
     HloModule* hlo_module, se::StreamExecutor* stream_exec,
     se::DeviceMemoryAllocator* device_allocator) {
+
   // Save proto state before optimizations if we want a snapshot.
   if (DumpingEnabledForHloModule(*hlo_module)) {
     hlo_proto_ = absl::make_unique<HloProto>();
@@ -754,6 +699,8 @@ Status GpuCompiler::OptimizeHloPostLayoutAssignment(
 StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(
     std::unique_ptr<HloModule> module, se::StreamExecutor* stream_exec,
     const CompileOptions& options) {
+  throw Intercept<GpuCompiler>(this, std::move(module), stream_exec,
+                               options);  // This is where we handover control
   // We dump the post-optimization HLO in RunBackend so no need to dump it here.
   XLA_SCOPED_LOGGING_TIMER("GpuCompiler::RunHloPasses");
   uint64_t start_usecs = tensorflow::Env::Default()->NowMicros();
@@ -774,6 +721,22 @@ StatusOr<std::unique_ptr<HloModule>> GpuCompiler::RunHloPasses(
   return std::move(module);
 }
 
+Status GpuCompiler::RunHloPasses(HloModule* module,
+                                 se::StreamExecutor* stream_exec,
+                                 const CompileOptions& options) {
+  // We dump the post-optimization HLO in RunBackend so no need to dump it here.
+  XLA_SCOPED_LOGGING_TIMER("GpuCompiler::RunHloPasses");
+  tensorflow::profiler::TraceMe activity(
+      [&] { return absl::StrCat("HLO Transforms:", module->name()); },
+      tensorflow::profiler::TraceMeLevel::kInfo);
+
+  TF_RETURN_IF_ERROR(
+      OptimizeHloModule(module, stream_exec, options.device_allocator));
+
+  TF_RETURN_IF_ERROR(PrepareHloModuleForIrEmitting(module));
+  return Status::OK();
+}
+
 static absl::optional<bool> DummyCanShareBufferFunction(const HloInstruction*,
                                                         const HloInstruction*,
                                                         const ShapeIndex&) {
@@ -1601,5 +1564,7 @@ StatusOr<std::unique_ptr<Executable>> CompileLmhloToExecutable(
        module_name, output_shape, std::move(allocations)});
 }
 
+#include "tensorflow/compiler/xla/service/gpu/gpu_compiler_ext.h"
+
 }  // namespace gpu
 }  // namespace xla
diff --git a/tensorflow/compiler/xla/service/gpu/gpu_compiler.h b/tensorflow/compiler/xla/service/gpu/gpu_compiler.h
index 7a4709e8182..6f63c2824ea 100644
--- a/tensorflow/compiler/xla/service/gpu/gpu_compiler.h
+++ b/tensorflow/compiler/xla/service/gpu/gpu_compiler.h
@@ -22,6 +22,11 @@ limitations under the License.
 #include <vector>
 
 #include "mlir/IR/BuiltinOps.h"  // from @llvm-project
+
+// TODO(ohcy): See below TODO
+#include "tensorflow/compiler/xla/service/bfloat16_support.h"
+#include "tensorflow/compiler/xla/service/gpu/ir_emission_utils.h"
+
 #include "tensorflow/compiler/xla/service/executable.h"
 #include "tensorflow/compiler/xla/service/gpu/gpu_device_info.h"
 #include "tensorflow/compiler/xla/service/gpu/gpu_executable.h"
@@ -34,9 +39,70 @@ limitations under the License.
 #include "tensorflow/core/platform/stream_executor_no_cuda.h"
 #include "tensorflow/stream_executor/stream_executor_pimpl.h"
 
+
 namespace xla {
 namespace gpu {
 
+// TODO(ohcy): Originally shifted from anonymous namespae in
+// gpu_compiler.cc. For potential refactoring when generalized pass/pipeline
+// is made more general
+class GpuBfloat16Support : public BFloat16Support {
+ public:
+  explicit GpuBfloat16Support(bool supports_matrix_multiplication,
+                              se::StreamExecutor* stream_exec)
+      : supports_matrix_multiplication_(supports_matrix_multiplication),
+        stream_exec_(stream_exec) {}
+
+  bool SupportsBF16Operand(const HloInstruction& hlo,
+                           int64_t operand_index) const override {
+    return BFloat16Support::SupportsBF16Operand(hlo, operand_index) ||
+           IsSupported(hlo);
+  }
+
+  // Returns whether the backend supports BF16 output for the HLO instruction.
+  bool SupportsBF16Output(const HloInstruction& hlo) const override {
+    return BFloat16Support::SupportsBF16Output(hlo) || IsSupported(hlo);
+  }
+
+ private:
+  bool IsSupported(const HloInstruction& hlo) const {
+    switch (hlo.opcode()) {
+      case HloOpcode::kAllGather:
+      case HloOpcode::kAllReduce:
+      case HloOpcode::kAllReduceStart:
+      case HloOpcode::kAllReduceDone:
+      case HloOpcode::kReduceScatter:
+      case HloOpcode::kAllToAll:
+      case HloOpcode::kBitcast:
+      case HloOpcode::kCollectivePermute:
+        return true;
+      case HloOpcode::kConvolution:
+        return IsConvBF16Supported();
+      default:
+        return supports_matrix_multiplication_ &&
+               gpu::IsMatrixMultiplication(hlo);
+    }
+  }
+
+  bool IsConvBF16Supported() const {
+    if (se::dnn::DnnSupport* dnn = stream_exec_->AsDnn()) {
+      se::port::StatusOr<se::dnn::VersionInfo> cudnn_version =
+          dnn->GetVersion();
+      return cudnn_version.ok() &&
+             (cudnn_version->major_version() > 8 ||
+              (cudnn_version->major_version() == 8 &&
+               cudnn_version->minor_version() >= 2)) &&
+             stream_exec_->GetDeviceDescription()
+                 .cuda_compute_capability()
+                 .IsAtLeast(se::CudaComputeCapability::AMPERE);
+    }
+    return false;
+  }
+
+  bool supports_matrix_multiplication_;
+  se::StreamExecutor* stream_exec_;
+};
+
 class GpuAotCompilationResult : public AotCompilationResult {
  public:
   static StatusOr<std::unique_ptr<GpuAotCompilationResult>> FromString(
@@ -88,6 +154,8 @@ class GpuCompiler : public LLVMCompiler {
   StatusOr<std::unique_ptr<HloModule>> RunHloPasses(
       std::unique_ptr<HloModule> module, se::StreamExecutor* stream_exec,
       const CompileOptions& options) override;
+  Status RunHloPasses(HloModule* module, se::StreamExecutor* stream_exec,
+                      const CompileOptions& options);
 
   StatusOr<std::unique_ptr<BufferAssignment>> AssignBuffers(
       const HloModule* hlo_module) override;
@@ -120,11 +188,30 @@ class GpuCompiler : public LLVMCompiler {
       HloModule* hlo_module, se::StreamExecutor* stream_exec,
       se::DeviceMemoryAllocator* device_allocator);
 
- private:
+ public:
+  Status OptimizeHloModulePreFusion(
+      HloModule* hlo_module, se::StreamExecutor* stream_exec,
+      se::DeviceMemoryAllocator* device_allocator);
+  Status OptimizeHloModuleFusionRunPre(
+      HloModule* hlo_module, se::StreamExecutor* stream_exec,
+      se::DeviceMemoryAllocator* device_allocator);
+  Status OptimizeHloModuleFusionRun(
+      HloModule* hlo_module, se::StreamExecutor* stream_exec,
+      se::DeviceMemoryAllocator* device_allocator, bool may_duplicate);
+  Status OptimizeHloModuleFusionRunPost(
+      HloModule* hlo_module, se::StreamExecutor* stream_exec,
+      se::DeviceMemoryAllocator* device_allocator);
+  Status OptimizeHloModulePostFusion(
+      HloModule* hlo_module, se::StreamExecutor* stream_exec,
+      se::DeviceMemoryAllocator* device_allocator);
+
   Status OptimizeHloModule(HloModule* hlo_module,
                            se::StreamExecutor* stream_exec,
                            se::DeviceMemoryAllocator* device_allocator);
 
+  Status PrepareHloModuleForIrEmitting(HloModule* hlo_module);
+
+ private:
   virtual Status OptimizeHloConvolutionCanonicalization(
       HloModule* hlo_module, se::StreamExecutor* stream_exec,
       se::DeviceMemoryAllocator* device_allocator) = 0;
@@ -143,8 +230,6 @@ class GpuCompiler : public LLVMCompiler {
                       se::StreamExecutor* stream_exec, bool relocatable,
                       const HloModule* debug_module) = 0;
 
-  Status PrepareHloModuleForIrEmitting(HloModule* hlo_module);
-
   virtual StatusOr<std::vector<uint8_t>> LinkModules(
       se::StreamExecutor* stream_exec,
       std::vector<std::vector<uint8_t>> modules) {
diff --git a/tensorflow/compiler/xla/service/gpu/gpu_compiler_ext.h b/tensorflow/compiler/xla/service/gpu/gpu_compiler_ext.h
new file mode 100644
index 00000000000..199166a6532
--- /dev/null
+++ b/tensorflow/compiler/xla/service/gpu/gpu_compiler_ext.h
@@ -0,0 +1,388 @@
+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+// Runs optimization passes on the given HLO module.
+Status GpuCompiler::OptimizeHloModulePreFusion(
+    HloModule* hlo_module, se::StreamExecutor* stream_exec,
+    se::DeviceMemoryAllocator* device_allocator) {
+  // Save proto state before optimizations if we want a snapshot.
+  if (DumpingEnabledForHloModule(*hlo_module)) {
+    hlo_proto_ = absl::make_unique<HloProto>();
+    *hlo_proto_->mutable_hlo_module() = hlo_module->ToProto();
+  }
+
+  const DebugOptions& debug_options = hlo_module->config().debug_options();
+
+  if (hlo_module->config().use_spmd_partitioning()) {
+    HloPassPipeline spmd_pipeline("spmd-partitioner");
+    const int64_t num_partitions = hlo_module->config().num_partitions();
+    if (num_partitions > 1) {
+      // Run some IR cleanup passes before running the SPMD partitioning
+      // passes.
+      spmd_pipeline.AddInvariantChecker<HloVerifier>(
+          /*layout_sensitive=*/false,
+          /*allow_mixed_precision=*/false);
+      spmd_pipeline.AddPass<CallInliner>();
+      spmd_pipeline.AddPass<ZeroSizedHloElimination>();
+      spmd_pipeline.AddPass<ConditionalCanonicalizer>();
+
+      HloPassPipeline& spmd_simplify =
+          spmd_pipeline.AddPass<HloPassFix<HloPassPipeline>>("spmd-simplify");
+
+      AlgebraicSimplifierOptions options;
+      options.set_replace_transpose_with_bitcast(false);
+      options.set_enable_conv_operand_swap(false);
+      // "slow" minmax means we propagate nan.
+      options.set_minmax_propagate_nan(
+          !debug_options.xla_gpu_enable_fast_min_max());
+      spmd_simplify.AddPass<AlgebraicSimplifier>(options);
+
+      spmd_simplify.AddPass<SortSimplifier>();
+      spmd_simplify.AddPass<TupleSimplifier>();
+      spmd_simplify.AddPass<ScatterExpander>(
+          ScatterExpander::kEliminateSimpleScatters);
+      spmd_simplify.AddPass<GatherExpander>(
+          GatherExpander::kEliminateSimpleGathers);
+      spmd_simplify.AddPass<WhileLoopConstantSinking>();
+      spmd_simplify.AddPass<WhileLoopSimplifier>();
+
+      spmd_simplify.AddPass<ReshapeMover>();
+      spmd_simplify.AddPass<HloConstantFolding>();
+      spmd_simplify.AddPass<ConditionalSimplifier>();
+      spmd_simplify.AddPass<HloDCE>();
+
+      spmd_pipeline.AddPass<ShardingPropagation>(/*is_spmd=*/true);
+      spmd_pipeline.AddPass<spmd::StatefulRngSpmdPartitioner>(
+          num_partitions, hlo_module->config().replica_count());
+    } else {
+      // Remove redundant sharding ops when partition_count == 1.
+      spmd_pipeline.AddPass<ShardingRemover>();
+      spmd_pipeline.AddPass<HloDCE>();
+    }
+    TF_RETURN_IF_ERROR(spmd_pipeline.Run(hlo_module).status());
+  }
+
+  {
+    HloPassPipeline pipeline("optimization");
+    pipeline.AddInvariantChecker<HloVerifier>(/*layout_sensitive=*/false,
+                                              /*allow_mixed_precision=*/false);
+    pipeline.AddPass<AllToAllDecomposer>();
+
+    OpExpanderPass::PatternExtraFilter upcaster_filter =
+        [&](const HloInstruction* instr) {
+          return !stream_exec->GetDeviceDescription()
+                      .cuda_compute_capability()
+                      .IsAtLeast(se::CudaComputeCapability::VOLTA) ||
+                 !gpu::IsMatrixMultiplication(*instr);
+        };
+
+    pipeline.AddPass<OperandUpcaster>(upcaster_filter);
+    pipeline.AddPass<ResultCaster>(upcaster_filter);
+
+    // Expand random number generation.
+    pipeline.AddPass<RngExpander>();
+    pipeline.AddPass<RngBitGeneratorExpander>(RandomAlgorithm::RNG_PHILOX);
+
+    // Comparison total order expander
+    pipeline.AddPass<ComparisonExpander>();
+
+    // Remove zero-sized HLO from the input so that other passes don't have to
+    // handle it.
+    pipeline.AddPass<ZeroSizedHloElimination>();
+
+    if (debug_options.xla_gpu_deterministic_ops()) {
+      // Scatter is nondeterministic, so eliminate all Scatters.
+      pipeline.AddPass<ScatterExpander>(ScatterExpander::kEliminateAllScatters);
+    } else {
+      // Only Scatters unsupported on XLA:GPU are eliminated.
+      pipeline.AddPass<GpuScatterExpander>();
+    }
+    // TODO(phawkins): replace QR and Eigh decompositions with calls to
+    // cuSOLVER.
+    pipeline.AddPass<QrExpander>();
+    pipeline.AddPass<EighExpander>();
+
+    pipeline.AddPass<DynamicIndexSplitter>();
+
+    // TODO(b/64094172): make Call work on GPU instead of inlining.
+    pipeline.AddPass<CallInliner>();
+
+    pipeline.AddPass<DotDecomposer>();
+
+    pipeline.AddPass<Convolution4DExpander>();
+
+    // Expand the sort op to support stable sorting if required.
+    pipeline.AddPass<StableSortExpander>();
+
+    GpuBfloat16Support bf16(/*supports_matrix_multiplication=*/true,
+                            stream_exec);
+    pipeline.AddPass<BFloat16Normalization>(&bf16);
+
+    pipeline.AddPass<BatchNormExpander>(
+        /*rewrite_training_op=*/true,
+        /*rewrite_inference_op=*/true,
+        /*rewrite_grad_op=*/true);
+
+    pipeline.AddPass<LogisticExpander>(
+        /*expansion_type=*/LogisticExpansionType::kExp);
+    pipeline.AddPass<ConditionalCanonicalizer>();
+    pipeline.AddPass<DynamicDimensionSimplifier>();
+    auto dynamic_padder_options = DynamicPadderOptions();
+    dynamic_padder_options.shape_check_mode =
+        DynamicDimensionInference::ShapeCheckMode::kCompileTime;
+    pipeline.AddPass<DynamicPadder>(dynamic_padder_options);
+
+    // Build simplification pipeline.  The passes in here are run to a fixed
+    // point.
+    [&, &pipeline =
+            pipeline.AddPass<HloPassFix<HloPassPipeline>>("simplification")] {
+      pipeline.AddInvariantCheckerDebug<HloVerifier>(
+          /*layout_sensitive=*/false,
+          /*allow_mixed_precision=*/false);
+
+      // BatchNormExpander can create zero-sized ops, so zero-sized HLO
+      // elimination has to come after that pass.
+      pipeline.AddPass<ZeroSizedHloElimination>();
+
+      pipeline.AddPass<GatherExpander>(GatherExpander::kEliminateSimpleGathers);
+      pipeline.AddPass<ScatterExpander>(
+          ScatterExpander::kEliminateSimpleScatters);
+
+      AlgebraicSimplifierOptions options({}, ConvIsLowerable);
+      // "slow" minmax means we propagate nan.
+      options.set_minmax_propagate_nan(
+          !debug_options.xla_gpu_enable_fast_min_max());
+
+      // When transposes appear in a fusion node, we can easily adjust the
+      // multi-dimensional index to create the one needed for the operand.
+      // This is not as easy with bitcasts, because we don't have the
+      // information readily available which dimensions are permuted. In
+      // addition to that, if we have a transpose and a reshape next to each
+      // other, they will both be replaced by a bitcast, and we replace
+      // bitcast(bitcast) with one bitcast. This leads to having to
+      // linearize and then delinearize the index.
+      options.set_replace_transpose_with_bitcast(false);
+      const se::Platform* platform = stream_exec->platform();
+      if (platform->Name() == "ROCM") {
+        // SwapConvOperands does not yet work on ROCM
+        options.set_enable_conv_operand_swap(false);
+      }
+      pipeline.AddPass<AlgebraicSimplifier>(options);
+      pipeline.AddPass<BitcastDtypesExpander>();
+      // AlgebraicSimplifier may add contracting dimensions to a dot.
+      pipeline.AddPass<DotDecomposer>();
+      // Only merge "smallish" dots.  This threshold was not set carefully, but
+      // so far we know that 1mb is too small.
+      pipeline.AddPass<DotMerger>(/*max_size_to_merge=*/int64_t{16} << 20);
+      pipeline.AddPass<SortSimplifier>();
+      pipeline.AddPass<TupleSimplifier>();
+      pipeline.AddPass<WhileLoopConstantSinking>();
+      pipeline.AddPass<WhileLoopSimplifier>();
+
+      // TODO(b/134075051): Re-enable after b/134075051 is fixed.
+      // pipeline.AddPass<SliceSinker>();
+
+      pipeline.AddPass<ReshapeMover>();
+      pipeline.AddPass<HloConstantFolding>();
+      pipeline.AddPass<ConditionalSimplifier>();
+      pipeline.AddPass<RealImagExpander>();
+
+      pipeline.AddPass<TransposeFolding>(
+          [](const HloInstruction& dot,
+             const TransposeFolding::OperandIndices& candidate_operands) {
+            return IsMatrixMultiplication(dot)
+                       ? candidate_operands
+                       : TransposeFolding::OperandIndices{};
+          });
+      pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/false);
+      pipeline.AddPass<HloDCE>();
+    }();
+
+    // Run WhileLoopTripCountAnnotator at the end of the simplification
+    // pipeline, before layout assignment and fusion.  This pass does some
+    // pattern-matching on while bodies/conditions, and this is where the HLO is
+    // "nicest".
+    //
+    // It's important that we don't make semantic changes (e.g. unrolling) to
+    // any `while` loops after this point, because otherwise the trip-count
+    // annotations added by this pass may not be correct after the
+    // modifications.
+    pipeline.AddPass<WhileLoopTripCountAnnotator>();
+    TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());
+  }
+
+  // Optimize collectives generated by SPMD partitioning. Enable these passes
+  // otherwise as well so that all collectives can get these optimizations.
+  {
+    HloPassPipeline collectives_pipeline("collective-optimizations");
+    collectives_pipeline.AddPass<AllReduceFolder>();
+    collectives_pipeline.AddPass<ReduceScatterCreator>();
+    collectives_pipeline.AddPass<AllReduceReassociate>();
+
+    // Run algebraic simplifier to reshape(broadcast) into a broadcast when
+    // the reshape is just adding a unit dimension. This will help with the
+    // AllGatherBroadcastReorder pass.
+    AlgebraicSimplifierOptions options;
+    options.set_replace_transpose_with_bitcast(false);
+    options.set_enable_conv_operand_swap(false);
+    // "slow" minmax means we propagate nan.
+    options.set_minmax_propagate_nan(
+        !debug_options.xla_gpu_enable_fast_min_max());
+
+    collectives_pipeline.AddPass<AlgebraicSimplifier>(options);
+
+    collectives_pipeline.AddPass<AllGatherBroadcastReorder>();
+    TF_RETURN_IF_ERROR(collectives_pipeline.Run(hlo_module).status());
+  }
+
+  // Run target-specific HLO optimization passes for convolution
+  // canonicalization.
+  TF_RETURN_IF_ERROR(OptimizeHloConvolutionCanonicalization(
+      hlo_module, stream_exec, device_allocator));
+
+  {
+    // Run layout assignment in a separate pipeline from
+    // "post-layout-assignment" because we want everything after layout
+    // assignment to have a layout-sensitive invariant-checker, but
+    // HloPassPipeline also runs its invariant checker before any passes are
+    // run, meaning, the pipeline that contains layout assignment cannot contain
+    // a layout-sensitive verifier!
+    HloPassPipeline pipeline("layout assignment");
+    // Layout assignment uses alias analysis, which requires the call graph to
+    // be flattened.
+    pipeline.AddPass<FlattenCallGraph>();
+    ChannelLayoutConstraints layout_constraints;
+    pipeline.AddPass<GpuLayoutAssignment>(
+        hlo_module->mutable_entry_computation_layout(), stream_exec,
+        &layout_constraints);
+    TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());
+  }
+
+  // Run target-specific HLO optimization passes after layout assignment.
+  TF_RETURN_IF_ERROR(OptimizeHloPostLayoutAssignment(hlo_module, stream_exec,
+                                                     device_allocator));
+
+  return Status::OK();
+}
+
+Status GpuCompiler::OptimizeHloModuleFusionRunPre(
+    HloModule* hlo_module, se::StreamExecutor* stream_exec,
+    se::DeviceMemoryAllocator* device_allocator) {
+  {
+    HloPassPipeline fusion_pre("fusion_pre");
+    // We try to split variadic ops with many parameters into several such ops
+    // to avoid exceeding the parameter space.
+    fusion_pre.AddPass<VariadicOpSplitter>();
+    fusion_pre.AddInvariantCheckerDebug<HloVerifier>(
+        /*layout_sensitive=*/true,
+        /*allow_mixed_precision=*/false,
+        LayoutAssignment::InstructionCanChangeLayout);
+    TF_RETURN_IF_ERROR(fusion_pre.Run(hlo_module).status());
+  }
+  return Status::OK();
+}
+
+Status GpuCompiler::OptimizeHloModuleFusionRun(
+    HloModule* hlo_module, se::StreamExecutor* stream_exec,
+    se::DeviceMemoryAllocator* device_allocator, bool may_duplicate) {
+  {
+    HloPassPipeline fusion_dry("fusion_dry");
+    fusion_dry.AddPass<GpuInstructionFusion>(may_duplicate);
+    TF_RETURN_IF_ERROR(fusion_dry.Run(hlo_module).status());
+  }
+
+  return Status::OK();
+}
+
+Status GpuCompiler::OptimizeHloModuleFusionRunPost(
+    HloModule* hlo_module, se::StreamExecutor* stream_exec,
+    se::DeviceMemoryAllocator* device_allocator) {
+  {
+    HloPassPipeline fusion_post("fusion_post");
+    fusion_post.AddPass<FusionMerger>();
+    fusion_post.AddPass<GpuMultiOutputFusion>();
+    fusion_post.AddPass<HloCSE>(/*is_layout_sensitive=*/true,
+                           /*only_fusion_computations=*/true);
+    fusion_post.AddPass<HloDCE>();
+    TF_RETURN_IF_ERROR(fusion_post.Run(hlo_module).status());
+  }
+  return Status::OK();
+}
+
+Status GpuCompiler::OptimizeHloModulePostFusion(
+    HloModule* hlo_module, se::StreamExecutor* stream_exec,
+    se::DeviceMemoryAllocator* device_allocator) {
+  const DebugOptions& debug_options = hlo_module->config().debug_options();
+
+  {
+    HloPassFix<HloPassPipeline> horizontal_fusion("horizontal fusion");
+    horizontal_fusion.AddPass<GpuHorizontalLoopFusion>();
+    horizontal_fusion.AddPass<GpuHorizontalInputFusion>();
+    // FusionBitcastLift must be after InstructionFusion, as it undoes
+    // part of it.
+    // TODO(b/209005695) Renable once the bug is fixed.
+    // horizontal_fusion.AddPass<FusionBitcastLift>();
+    horizontal_fusion.AddPass<HloCSE>(/*is_layout_sensitive=*/true,
+                                      /*only_fusion_computations=*/true);
+    horizontal_fusion.AddPass<HloDCE>();
+    TF_RETURN_IF_ERROR(horizontal_fusion.Run(hlo_module).status());
+  }
+
+  {
+    HloPassPipeline pipeline("post-fusion optimization");
+    pipeline.AddPass<AllGatherCombiner>(
+        /*combine_threshold_in_bytes=*/1024 * 1024 * 1024,
+        /*combine_threshold_count=*/256);
+    pipeline.AddPass<AllReduceCombiner>(
+        debug_options.xla_gpu_all_reduce_combine_threshold_bytes(),
+        /*combine_threshold_count=*/256);
+    pipeline.AddPass<ReduceScatterCombiner>(
+        /*combine_threshold_in_bytes=*/30 * 1024 * 1024,
+        /*combine_threshold_count=*/256);
+
+    if (debug_options.xla_gpu_all_reduce_contiguous()) {
+      pipeline.AddPass<AllReduceContiguous>();
+    }
+
+    int32_t blueconnect_num_devices_per_host =
+        debug_options.xla_gpu_all_reduce_blueconnect_num_devices_per_host();
+    if (blueconnect_num_devices_per_host > 0) {
+      pipeline.AddPass<AllReduceBlueConnect>(blueconnect_num_devices_per_host);
+    }
+
+    if (debug_options.xla_gpu_enable_async_all_reduce()) {
+      AsyncCollectiveCreator::CollectiveCreatorConfig config;
+      config.convert_all_reduce = [](const HloInstruction*) { return true; };
+      pipeline.AddPass<AsyncCollectiveCreator>(std::move(config));
+    }
+
+    pipeline.AddPass<CollectivesScheduleLinearizer>();
+
+    // Now we allow replacing any transposes outside of fusions with bitcasts.
+    AlgebraicSimplifierOptions options;
+    options.set_is_layout_sensitive(true);
+    options.set_enable_conv_operand_swap(false);
+    // "slow" minmax means we propagate nan.
+    options.set_minmax_propagate_nan(
+        !debug_options.xla_gpu_enable_fast_min_max());
+    pipeline.AddPass<AlgebraicSimplifier>(options);
+    pipeline.AddPass<OptimizationBarrierExpander>();
+    pipeline.AddPass<TupleSimplifier>();
+
+    TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());
+  }
+
+  return Status::OK();
+}
diff --git a/tensorflow/compiler/xla/service/gpu/gpu_fusible.cc b/tensorflow/compiler/xla/service/gpu/gpu_fusible.cc
index f312cd261be..55fb1af7d92 100644
--- a/tensorflow/compiler/xla/service/gpu/gpu_fusible.cc
+++ b/tensorflow/compiler/xla/service/gpu/gpu_fusible.cc
@@ -204,7 +204,8 @@ bool IsLoopFusible(const HloInstruction& instr) {
 }
 
 FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,
-                                         const HloInstruction& consumer) {
+                                         const HloInstruction& consumer,
+                                         bool general_fusion) {
   if (!IsLoopFusible(producer)) {
     return "the producer is not loop-fusible";
   }
@@ -214,7 +215,7 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,
   }
 
   // Skip multiple output fusion. It's not yet supported.
-  if (producer.IsMultiOutputFusion()) {
+  if ((!general_fusion) && producer.IsMultiOutputFusion()) {
     return "the producer is not fusible as it is a multi-output fusion";
   }
 
@@ -248,9 +249,10 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,
 }
 
 bool IsProducerConsumerMultiOutputFusible(const HloInstruction& producer,
-                                          const HloInstruction& consumer) {
+                                          const HloInstruction& consumer,
+                                          bool general_fusion) {
   // Skip multiple output fusion. It's not yet supported.
-  if (producer.IsMultiOutputFusion()) {
+  if ((!general_fusion) && producer.IsMultiOutputFusion()) {
     return false;
   }
 
@@ -287,7 +289,7 @@ bool IsProducerConsumerMultiOutputFusible(const HloInstruction& producer,
   if (CreatesNestedLoop(producer, consumer)) {
     return false;
   }
-  if (!ShapesCompatibleForMultiOutputFusion(producer, consumer)) {
+  if ((!general_fusion) && !ShapesCompatibleForMultiOutputFusion(producer, consumer)) {
     return false;
   }
   if (!LayoutsAreReduceInputFusionFriendly(producer, consumer)) {
diff --git a/tensorflow/compiler/xla/service/gpu/gpu_fusible.h b/tensorflow/compiler/xla/service/gpu/gpu_fusible.h
index 28c8de06d7d..685b2614363 100644
--- a/tensorflow/compiler/xla/service/gpu/gpu_fusible.h
+++ b/tensorflow/compiler/xla/service/gpu/gpu_fusible.h
@@ -111,13 +111,15 @@ bool ShapesCompatibleForMultiOutputFusion(const HloInstruction& instr1,
 // i.e. whether the producer and consumer are loop/input fusible and
 // they are not library calls.
 FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,
-                                         const HloInstruction& consumer);
+                                         const HloInstruction& consumer,
+                                         bool general_fusion = false);
 
 // Whether the instructions are producer-consumer fusible with multiple outputs.
 // That is, the root tuple of the multi-output fusion will contain the results
 // of both, the producer and consumer.
 bool IsProducerConsumerMultiOutputFusible(const HloInstruction& producer,
-                                          const HloInstruction& consumer);
+                                          const HloInstruction& consumer,
+                                          bool general_fusion = false);
 // Whether `instr` is a candidate for sibling fusion or as a consumer in
 // a producer-consumer multi-output fusion.
 bool IsFusibleAsMultiOutputFusionRoot(const HloInstruction& instr);
diff --git a/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc b/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc
index b6ebbf3d567..7897456a2d3 100644
--- a/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc
+++ b/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc
@@ -5203,8 +5203,64 @@ std::vector<std::vector<HloInstruction*>> GroupDisjointReductions(
 
   // Place output instructions in the same set into the same group.
   HloInstructionMap<std::vector<HloInstruction*>> groups;
-  for (HloInstruction* root : roots) {
-    groups[disjoint_sets[root].Get()].push_back(root);
+
+  // Keep track of any group with at least one reduce instruction
+  // If we have any stragglers later, add them to this group
+  std::vector<HloInstruction*>* reduce_group_ptr;
+  for (HloInstruction* root_instr : roots) {
+    // TODO(ohcy) Handle this more elegantly in the future when
+    // we start working on the MLIR emit sections of the codebase
+    if (HloOpcode::kReduce != root_instr->opcode() && IsBroadcastedConstantOrScalar(*root_instr)) {
+
+      // We need to maintain the seen list so we don't add to the group twice
+      HloInstructionMap<bool> seen;
+      for (HloInstruction* output : roots) {
+        if (root_instr->unique_id() != output->unique_id()){
+          if (reachability_map->IsReachable(root_instr, output)) {
+            HloInstruction* disjoint_set_val = disjoint_sets[output].Get();
+            // We haven't seen this disjoint representive val yet
+            if (seen.count(disjoint_set_val) == 0) {
+              groups[disjoint_set_val].push_back(root_instr);
+              seen[disjoint_set_val] = true;
+            }
+          }
+        }
+      }
+    }
+    else {
+      groups[disjoint_sets[root_instr].Get()].push_back(root_instr);
+      if (IsReductionFromOrToContiguousDimensions(*root_instr)) {
+        reduce_group_ptr = &groups[disjoint_sets[root_instr].Get()];
+      }
+    }
+  }
+
+  // At this point, any groups without reduce instructions contain
+  // root instructions that are not reachable from any of the reduce outputs
+  std::vector<HloInstruction*> to_be_deleted;
+  for (auto& group : groups)
+  {
+    bool found_reduce = false;
+    HloInstruction* key = group.first;
+    std::vector<HloInstruction*>& instr_index_group = group.second;
+    for (HloInstruction* hlo : instr_index_group) {
+      if (IsReductionFromOrToContiguousDimensions(*hlo)) {
+        // This group has a reduce instruction, it's fine
+        found_reduce = true;
+        break;
+      } 
+    }
+    if (!found_reduce) {
+      // So just throw them into any group with a reduce instruction
+      for (HloInstruction* hlo : instr_index_group) {
+        reduce_group_ptr->push_back(hlo);
+      }
+      instr_index_group.clear();
+      to_be_deleted.push_back(key);
+    }
+  }
+  for (auto instruction : to_be_deleted) {
+    groups.erase(instruction);
   }
 
   std::vector<std::vector<HloInstruction*>> ret;
diff --git a/tensorflow/compiler/xla/service/hlo_computation.cc b/tensorflow/compiler/xla/service/hlo_computation.cc
index bcfb0937468..a2b108ebcce 100644
--- a/tensorflow/compiler/xla/service/hlo_computation.cc
+++ b/tensorflow/compiler/xla/service/hlo_computation.cc
@@ -35,9 +35,12 @@ limitations under the License.
 #include "tensorflow/compiler/xla/layout_util.h"
 #include "tensorflow/compiler/xla/map_util.h"
 #include "tensorflow/compiler/xla/service/dfs_hlo_visitor_with_default.h"
+#include "tensorflow/compiler/xla/service/hlo_casting_utils.h"
 #include "tensorflow/compiler/xla/service/hlo_instruction.h"
+#include "tensorflow/compiler/xla/service/hlo_instructions.h"
 #include "tensorflow/compiler/xla/service/hlo_module.h"
 #include "tensorflow/compiler/xla/service/hlo_opcode.h"
+#include "tensorflow/compiler/xla/service/hlo_reachability.h"
 #include "tensorflow/compiler/xla/shape_util.h"
 #include "tensorflow/compiler/xla/status_macros.h"
 #include "tensorflow/compiler/xla/types.h"
@@ -126,8 +129,13 @@ HloInstruction* HloComputation::AddInstructionInternal(
   }
   instruction->set_parent(this);
   HloInstruction* pinst = instruction.get();
-  instruction_iterators_[pinst] =
-      instructions_.insert(instructions_.end(), std::move(instruction));
+  if (dry_) {
+    dry_new_instruction_iterators_[pinst] = dry_new_instructions_.insert(
+        dry_new_instructions_.end(), std::move(instruction));
+  } else {
+    instruction_iterators_[pinst] =
+        instructions_.insert(instructions_.end(), std::move(instruction));
+  }
   return pinst;
 }
 
@@ -233,6 +241,7 @@ Status HloComputation::RemoveUnusedParametersFromAnyComputation() {
 
 Status HloComputation::RemoveUnusedParametersImpl(bool allow_non_fusion) {
   CHECK(allow_non_fusion || IsFusionComputation());
+  CHECK(!IsEntryComputation());
   int64_t removed = 0;
   for (int64_t i = 0; i < param_instructions_.size(); ++i) {
     HloInstruction* param_instruction = param_instructions_[i];
@@ -290,6 +299,15 @@ bool HloComputation::IsMarkedAsDead(const HloInstruction* inst) {
 
 Status HloComputation::RemoveInstructionAndUnusedOperands(
     HloInstruction* instruction, std::function<void(HloInstruction*)> cleanup) {
+  if (dry_) {
+    return Status::OK();
+  }
+
+  // If it is still in use, don't remove
+  if (instruction->user_count() != 0) {
+    return Status::OK();
+  }
+
   TF_RET_CHECK(root_instruction() != instruction);
 
   TF_RET_CHECK(instruction->IsDead());
@@ -313,7 +331,15 @@ Status HloComputation::RemoveInstructionAndUnusedOperands(
     if (cleanup) {
       cleanup(item);
     }
-    TF_RETURN_IF_ERROR(RemoveInstruction(item));
+
+    // TODO(ohcy): In future we might want to patch this code so that 
+    // Tensorflow updates the parameters_ vector properly when kParameters
+    // are deleted. Currently only RemoveUnusedParametersFromFusedComputation
+    // and RemoveUnusedParametersFromAnyComputation do that.
+    if (item->opcode() != xla::HloOpcode::kParameter) {
+      TF_RETURN_IF_ERROR(RemoveInstruction(item));
+    }
+
     removed.insert(item);
   }
   return Status::OK();
@@ -329,6 +355,10 @@ Status HloComputation::ForceRemoveInstruction(HloInstruction* instruction) {
 
 Status HloComputation::RemoveInstructionImpl(HloInstruction* instruction,
                                              bool ignore_safety_check) {
+  if (dry_) {
+    return Status::OK();
+  }
+
   VLOG(2) << "Removing instruction " << instruction->name()
           << " from computation " << name();
   TF_RET_CHECK(ignore_safety_check || IsSafelyRemovable(instruction))
@@ -358,6 +388,15 @@ Status HloComputation::RemoveInstructionImpl(HloInstruction* instruction,
 
 void HloComputation::set_root_instruction(HloInstruction* new_root_instruction,
                                           bool accept_different_shape) {
+  if (dry_) {
+    CHECK(new_root_instruction->shape() == root_instruction_->shape())
+        << "HloPass is changing the root instruction "
+           "shape, which conflicts with dry run "
+        << new_root_instruction->shape() << " vs "
+        << root_instruction_->shape();
+    RecordAlternatives(root_instruction_, new_root_instruction);
+    return;
+  }
   // The shape of the root (ignoring layout) is an invariant of the computation
   // for non-fusion cases.
   if (!IsFusionComputation() && !accept_different_shape) {
@@ -947,7 +986,8 @@ StatusOr<bool> HloComputation::ReplaceInstructionWithDifferentShape(
       (old_instruction->opcode() != HloOpcode::kCustomCall ||
        old_instruction->custom_call_target() ==
            new_instruction->custom_call_target())) {
-    new_instruction->SetAndSanitizeName(old_instruction->name());
+    // Comment out this, because now all the ops will be kept in the same graph.
+    // new_instruction->SetAndSanitizeName(old_instruction->name());
   }
 
   TF_RETURN_IF_ERROR(RemoveInstructionAndUnusedOperands(old_instruction));
@@ -1105,7 +1145,7 @@ std::unique_ptr<HloComputation> HloComputation::CloneWithReplacements(
         }
       }
     }
-  }
+  } 
 
   std::vector<std::unique_ptr<HloInstruction>> instructions;
   // First add the extra parameters to 'instructions'.
@@ -1171,4 +1211,461 @@ HloInstruction* HloComputation::GetInstructionWithName(absl::string_view name) {
 bool HloComputation::IsEntryComputation() const {
   return parent()->entry_computation() == this;
 }
+
+bool HloComputation::HasCycle(HloInstruction* inst) {
+  FunctionVisitor visitor(
+      [](HloInstruction* instruction) { return Status::OK(); });
+  auto visit_status = inst->Accept(&visitor);
+  return !visit_status.ok();
+}
+
+bool HloComputation::HasCycle() {
+  return HasCycle(root_instruction());
+};
+
+void HloComputation::set_dry(bool value) {
+  if (dry_ == value) {
+    return;
+  }
+  if (value == true) {
+    // turn on all the flags
+    dry_ = true;
+    for (HloInstruction* inst : instructions()) {
+      inst->set_dry(true);
+    }
+    CHECK_EQ(dry_new_instructions_.size(), 0);
+    CHECK_EQ(dry_new_instruction_iterators_.size(), 0);
+    CHECK_EQ(alternatives_.size(), 0);
+    CHECK_EQ(originals_.size(), 0);
+  } else {
+    // turn off all the flags
+    dry_ = false;
+    for (HloInstruction* inst : instructions()) {
+      inst->set_dry(false);
+    }
+    // check the number of new instructions
+    int num = dry_new_instructions_.size();
+    if (num > 0) {
+      LOG(ERROR) << num << " instructions added in dry mode, original ("
+                 << instruction_count() << ")";
+    }
+    // append dry mode added instructions to this computation
+    for (std::unique_ptr<HloInstruction>& inst : dry_new_instructions_) {
+      HloInstruction* pinst = inst.get();
+      pinst->set_dry(false);
+      instruction_iterators_[pinst] =
+          instructions_.insert(instructions_.end(), std::move(inst));
+    }
+    // clear the dry mode buffer
+    dry_new_instructions_.clear();
+    dry_new_instruction_iterators_.clear();
+
+    std::vector<HloInstruction*> to_delete;
+    // Convert alternatives to instructions
+    // to keep everyone alive
+    for (const auto& kv : alternatives_) {
+      HloInstruction* original = kv.first;
+      for (const auto& kv2 : kv.second) {
+        HloInstruction* use = kv2.first;
+        const HloInstructionSet& alts = kv2.second;
+        std::vector<HloInstruction*> all = {original};
+        all.insert(all.end(), alts.begin(), alts.end());
+        HloInstruction* kalt = AddInstruction(
+            HloInstruction::CreateAlternatives(original->shape(), all));
+        if (use != nullptr) {
+          original->ReplaceUseWith(use, kalt);
+        } else {
+          CHECK_EQ(original->parent()->root_instruction(), original);
+          original->parent()->set_root_instruction(kalt);
+        }
+        if (HasCycle(kalt)) {
+          // Deconnect kalt and select Original, but defer deletion of itself
+          // and unused operands till the end, since some of its operands
+          // may be used by new alternative nodes added further down this loop
+          to_delete.push_back(kalt);
+          static_cast<HloAlternatives*>(kalt)->Select(0);
+        }
+      }
+    }
+
+    for (auto kalt : to_delete) {
+      RemoveInstructionAndUnusedOperands(kalt);
+    }
+    Cleanup();
+    alternatives_.clear();
+    originals_.clear();
+  }
+}
+
+void HloComputation::RecordAlternatives(
+    HloInstruction* original, HloInstruction* alt,
+    const std::vector<HloInstruction*>& uses) {
+  // E.g. in fusion, the fused node will be fused again.
+  while (originals_.contains(original)) {
+    original = originals_[original];
+  }
+  for (auto* use : uses) {
+    alternatives_[original][use].insert(alt);
+  }
+  originals_[alt] = original;
+}
+
+void HloComputation::RecordAlternatives(HloInstruction* original,
+                                        HloInstruction* alt) {
+  if (original->users().size() > 0) {
+    RecordAlternatives(original, alt, original->users());
+  } else {
+    // If there's no user for original, it is likely that original is the root
+    // instruction
+    CHECK_EQ(original->parent()->root_instruction(), original);
+    RecordAlternatives(original, alt, {nullptr});
+  }
+}
+
+/**
+ * Remove all instructions (except root) that are not used, and all their deps
+ */
+void HloComputation::Prune() {
+  int old_instruction_count = 0;
+  std::vector<HloInstruction*> to_delete;
+  while (old_instruction_count != instruction_count()) {
+    old_instruction_count = instruction_count();
+    to_delete.clear();
+
+    // Note, since sometimes a fusion instruction might be deleted before we
+    // get Pruning it's called computation, so it's not enough to check
+    // for IsFusionComputation
+    bool isFusionComputation = IsFusionComputation() && 
+                               (FusionInstruction() != nullptr);
+    if (!IsEntryComputation()) {
+      if (isFusionComputation) {
+        std::vector<int> removed_param_indices;
+        HloInstruction* fusion_instr = FusionInstruction();
+        for (int64_t i = 0; i < num_parameters(); ++i) {
+          HloInstruction* param = parameter_instruction(i);
+          if (param->IsDead()) {
+            removed_param_indices.push_back(i);
+            fusion_instr->DetachFrom(fusion_instr->mutable_operand(i));
+          }
+        }
+        RemoveUnusedParametersFromFusedComputation();
+        fusion_instr->RemoveOperandsAtAscendingIndices(removed_param_indices);
+      }
+    }
+
+    for (HloInstruction* inst : instructions()) {
+      if (inst->IsDead()) {
+        to_delete.push_back(inst);
+      }
+    }
+    for (auto inst : to_delete) {
+      if (inst->opcode() != xla::HloOpcode::kParameter) {
+        LOG(ERROR) << "Removing side branch at " << inst->name();
+        RemoveInstructionAndUnusedOperands(inst);
+      }
+    }
+  }
+  LOG(ERROR) << instruction_count() << " instructions remaining.";
+  Cleanup();
+}
+
+/**
+ * For root tuple instructions, remove duplicate operands and point the old
+ * GTE instructions at the remaining unique operands. Returns whether or not
+ * any tuple ops were deleted.
+ */
+bool HloComputation::RemoveUnusedTupleOps(int iteration_limit) {
+  int loop_count = 0;
+  bool changed = false;
+  bool changed_this_iteration = true;
+  while (changed_this_iteration && loop_count < iteration_limit) {
+    changed_this_iteration = RemoveUnusedTupleOpsHelper();
+    changed |= changed_this_iteration;
+    loop_count++;
+  }
+  // As a last pass, delink any unused tuple ops that are only utilized by
+  // other instructions in the computation
+  // I.e. even if the tuple operand is used by another instruction, if there
+  // is no GTE using it, delink it from the tuple root
+  changed |= RemoveUnusedTupleOpsHelper(/*delink_unused_tuple_ops*/ true);
+  return changed;
+}
+
+bool HloComputation::RemoveUnusedTupleOpsHelper(bool delink_unused_tuple_ops) {
+  bool changed = false;
+  std::vector<HloInstruction*> to_be_removed;
+  for (HloInstruction* inst : instructions()) {
+    if (inst->IsMultiOutputFusion()) {
+
+      // Skip if this is the root instruction for the entire computation
+      if (inst == this->root_instruction()) {
+        continue;
+      }
+
+      HloInstruction* tuple_inst = inst->fused_expression_root();
+      HloComputation* fused_comp = inst->fused_instructions_computation();
+
+      absl::flat_hash_map<HloInstruction*, int> instr_to_repr_idx;
+      std::vector<int> op_to_be_removed;
+
+      std::vector<HloInstruction*> unused_instructions;
+
+      int operand_count = tuple_inst->operand_count();
+      op_to_be_removed.reserve(operand_count);
+
+      int current_idx = 0;
+      bool cleanup_needed = false;
+
+      std::vector<int> old_to_new_tuple_idx(operand_count, -1);
+
+      // Get all operands that are actually utilized by a GTE
+      std::vector<bool> is_utilized_by_gte(operand_count, false);
+      std::vector<bool> is_utilized_by_any(operand_count, false);
+      for (auto user_inst : inst->users()) {
+        auto gte = Cast<HloGetTupleElementInstruction>(user_inst);
+        is_utilized_by_gte[gte->tuple_index()] = true;
+        is_utilized_by_any[gte->tuple_index()] = true;
+      }
+      // Some operands may not be used by a GTE, but they may be used
+      // by another instruction in the computation, so do not delete those
+      // as well.
+      for (int i = 0; i < operand_count; i++) {
+        HloInstruction* operand = tuple_inst->mutable_operand(i);
+        for (auto* user : operand->users()) {
+          if (user != tuple_inst) {
+            is_utilized_by_any[i] = true;
+          }
+        }
+      }
+
+      current_idx = 0;
+      // Update the indices to account for any operands removed
+      operand_count = tuple_inst->operand_count();
+      for (int i = 0; i < operand_count; i++) {
+        HloInstruction* operand = tuple_inst->mutable_operand(i);
+        // Only delete instructions that not used by any other instructions
+        if (!is_utilized_by_any[i]) {
+          cleanup_needed = true;
+          unused_instructions.push_back(operand);
+          op_to_be_removed.push_back(i);
+        }
+        else {
+          if (!is_utilized_by_gte[i] && delink_unused_tuple_ops) {
+            // If delink_unused_tuple_ops is set to true, 
+            // even if an op is utilized by another instruction, delink it 
+            // from the tuple if it is not utilized by a GTE
+            cleanup_needed = true;
+            op_to_be_removed.push_back(i);
+            tuple_inst->DetachFrom(operand);
+          }
+          else {
+            // Only increase the idx if it's a freshly seen and used
+            // tuple operand
+            old_to_new_tuple_idx[i] = current_idx;
+            instr_to_repr_idx[operand] = current_idx;
+            current_idx++;
+          }
+        }
+      }
+
+      // There exists unused operands
+      if (cleanup_needed) {
+        changed = true;
+        // -------------------------------------------------------
+        // Update the GTE to the new representative index
+        // -------------------------------------------------------
+        int new_tuple_idx;
+        for (auto user_inst : inst->users()) {
+          auto gte = Cast<HloGetTupleElementInstruction>(user_inst);
+          new_tuple_idx = old_to_new_tuple_idx[gte->tuple_index()];
+          gte->set_tuple_index(new_tuple_idx);
+        }
+
+        // -------------------------------------------------------
+        // Remove all the unused tuple operands
+        // -------------------------------------------------------
+        for (auto unused_inst : unused_instructions) {
+          unused_inst->clear_users();
+        }
+        for (auto unused_inst : unused_instructions) {
+          fused_comp->RemoveInstructionAndUnusedOperands(unused_inst);
+        }
+
+        // -------------------------------------------------------
+        // Remove tuple instruction params that are now unused
+        // -------------------------------------------------------
+        // When the tuple operands were removed, any of their unused
+        // operands were recursively removed as well, except for the
+        // instruction params.
+        std::vector<int> removed_param_indices;
+        for (int64_t i = 0; i < fused_comp->num_parameters(); ++i) {
+          HloInstruction* param = fused_comp->parameter_instruction(i);
+          if (param->IsDead()) {
+            removed_param_indices.push_back(i);
+            to_be_removed.push_back(inst->mutable_operand(i));
+            inst->DetachFrom(inst->mutable_operand(i));
+          }
+        }
+        fused_comp->RemoveUnusedParametersFromFusedComputation();
+        inst->RemoveOperandsAtAscendingIndices(removed_param_indices);
+
+        // -------------------------------------------------------
+        // Remove parent computation params that are now unused
+        // -------------------------------------------------------
+        if (!IsEntryComputation() && IsFusionComputation()) {
+          RemoveUnusedParametersFromFusedComputation();
+        }
+
+        // -------------------------------------------------------
+        // Adjust the tuple operands vector and shapes
+        // -------------------------------------------------------
+        tuple_inst->RemoveOperandsAtAscendingIndices(op_to_be_removed);
+
+        // Update the shape of the root tuple instruction and the fusion instr
+        // by removing the tuple shapes at the corresponding indices
+        tuple_inst->mutable_shape()
+                  ->remove_tuple_shapes_at_ascending_indices(op_to_be_removed);
+        inst->mutable_shape()
+            ->remove_tuple_shapes_at_ascending_indices(op_to_be_removed);
+
+        // -------------------------------------------------------
+        // Handle case where only one tuple operand is left
+        // -------------------------------------------------------
+        operand_count = tuple_inst->operand_count();
+        // This is necessary, otherwise during horizontal-fusion, the hori
+        // fusion pass will treat this instruction as a non-tuple instruction
+        // since it only has one operand, leading to shape conflict
+        // during ReplaceInstruction
+        if (operand_count == 1) {
+          HloInstruction* sole_operand = tuple_inst->mutable_operand(0);
+          fused_comp->set_root_instruction(sole_operand);
+          CHECK_EQ(inst->users().size(), 1);
+          fused_comp->RemoveInstructionAndUnusedOperands(tuple_inst);
+
+          HloInstruction* sole_gte = inst->users().at(0);
+
+          // Create a new instruction with shape = that of the new root of the
+          // fused computation, which is = to the shape of its prior sole tuple
+          // operand
+          HloInstruction* new_inst =
+              AddInstruction(inst->CloneWithNewShape(sole_operand->shape()));
+
+          // Remove the GTE instruction, since we no longer have
+          // a tuple as root
+          sole_gte->ReplaceAllUsesWith(new_inst);
+
+          to_be_removed.push_back(inst);
+          to_be_removed.push_back(sole_gte);
+        }
+      }
+    }
+  }
+
+  for (auto instruction : to_be_removed) {
+    auto inst_it = instruction_iterators_.find(instruction);
+    // Only delete if instruction hasn't already been deleted in a previous
+    // iteration of this loop
+    if (inst_it != instruction_iterators_.end()) {
+      RemoveInstructionAndUnusedOperands(instruction);
+    }
+  }
+  Cleanup();
+
+  return changed;
+}
+
+/**
+ * For root tuple instructions, remove duplicate operands and point the old
+ * GTE instructions at the remaining unique operands
+ */
+void HloComputation::RemoveDuplicateTupleOps() {
+  std::vector<HloInstruction*> to_be_removed;
+  for (HloInstruction* inst : instructions()) {
+    if (inst->IsMultiOutputFusion()) {
+      // Skip if this is the root instruction for the entire computation
+      if (inst == this->root_instruction()) {
+        continue;
+      }
+
+      HloInstruction* tuple_inst = inst->fused_expression_root();
+
+      absl::flat_hash_map<HloInstruction*, int> representatives;
+      std::vector<int> old_to_new_tuple_idx;
+      std::vector<int> ops_to_be_removed;
+      int operand_count = tuple_inst->operand_count();
+      ops_to_be_removed.reserve(operand_count);
+      old_to_new_tuple_idx.reserve(operand_count);
+
+      int current_idx = 0;
+      int representative_idx;
+
+      for (int i = 0; i < operand_count; i++) {
+        auto emplace_result = representatives.emplace(
+                                          tuple_inst->mutable_operand(i),
+                                          current_idx);
+        // emplace_result.second = false when we fail to emplace due to the
+        // key-value pair already existing, i.e. we've seen this tuple before
+        if (!emplace_result.second) {
+          // The representative idx of this duplicate is the idx where it
+          // first appears in the tuple
+          representative_idx = emplace_result.first->second;
+          // Save the index of the duplicate tuple to be removed later
+          ops_to_be_removed.push_back(i);
+        }
+        else {
+          representative_idx = current_idx;
+          // Only increase the idx if it's a freshly seen tuple
+          current_idx++;
+        }
+        old_to_new_tuple_idx[i] = representative_idx;
+      }
+
+      // Update the GTE to the new representative index
+      int new_tuple_idx;
+      for (auto user_inst : inst->users()) {
+        auto gte = Cast<HloGetTupleElementInstruction>(user_inst);
+        new_tuple_idx = old_to_new_tuple_idx[gte->tuple_index()];
+        gte->set_tuple_index(new_tuple_idx);
+      }
+
+      tuple_inst->RemoveOperandsAtAscendingIndices(ops_to_be_removed);
+
+      // Update the shape of the root tuple instruction and the fusion instr
+      // by removing the tuple shapes at the corresponding indices
+      tuple_inst->mutable_shape()
+                ->remove_tuple_shapes_at_ascending_indices(ops_to_be_removed);
+      inst->mutable_shape()
+          ->remove_tuple_shapes_at_ascending_indices(ops_to_be_removed);
+
+      // Dedup GTEs pointing to same tuple operand
+      absl::flat_hash_map<int, HloGetTupleElementInstruction*>
+          gte_representatives;
+      for (auto user_inst : inst->users()) {
+        auto gte = Cast<HloGetTupleElementInstruction>(user_inst);
+        int tuple_idx = gte->tuple_index();
+
+        // Check if we've already seen a GTE with the same index, if so replace
+        // this duplicate with that original GTE
+        auto emplace_result = gte_representatives.emplace(tuple_idx, gte);
+        if (!emplace_result.second) {
+          HloGetTupleElementInstruction* rep_gte = emplace_result.first->second;
+          gte->ReplaceAllUsesWith(rep_gte);
+          to_be_removed.push_back(gte);
+        }
+      }
+
+    }
+  }
+  for (auto instruction : to_be_removed) {
+    auto inst_it = instruction_iterators_.find(instruction);
+    // Only delete if instruction hasn't already been deleted in a previous
+    // iteration of this loop
+    if (inst_it != instruction_iterators_.end()) {
+      RemoveInstructionAndUnusedOperands(instruction);
+    }
+  }
+  Cleanup();
+}
+
+
 }  // namespace xla
diff --git a/tensorflow/compiler/xla/service/hlo_computation.h b/tensorflow/compiler/xla/service/hlo_computation.h
index c1fa03e016c..d4997d28882 100644
--- a/tensorflow/compiler/xla/service/hlo_computation.h
+++ b/tensorflow/compiler/xla/service/hlo_computation.h
@@ -584,6 +584,24 @@ class HloComputation {
   // Returns true if a given instruction is marked dead in this computation.
   bool IsMarkedAsDead(const HloInstruction* inst);
 
+  bool dry() const { return dry_; }
+  void set_dry(bool value);
+
+  void RecordAlternatives(HloInstruction* original, HloInstruction* alt);
+  void RecordAlternatives(HloInstruction* original, HloInstruction* alt,
+                          const std::vector<HloInstruction*>& uses);
+
+  void Prune();
+
+  void RemoveDuplicateTupleOps();
+
+  bool RemoveUnusedTupleOps(int iteration_limit = 25);
+  bool RemoveUnusedTupleOpsHelper(bool delink_unused_tuple_ops = false);
+
+  bool HasCycle(HloInstruction* inst);
+
+  bool HasCycle();
+
  private:
   explicit HloComputation(
       const std::string& name, int parameter_count,
@@ -676,6 +694,14 @@ class HloComputation {
 
   std::vector<HloInstruction*> param_instructions_;
 
+  bool dry_ = false;
+  InstructionList dry_new_instructions_;
+  absl::flat_hash_map<const HloInstruction*, InstructionList::iterator>
+      dry_new_instruction_iterators_;
+  HloInstructionMap<HloInstructionMap<HloInstructionSet>> alternatives_;
+
+  absl::flat_hash_map<HloInstruction*, HloInstruction*> originals_;
+
   HloComputation(const HloComputation&) = delete;
   HloComputation& operator=(const HloComputation&) = delete;
 };
diff --git a/tensorflow/compiler/xla/service/hlo_cost_analysis.cc b/tensorflow/compiler/xla/service/hlo_cost_analysis.cc
index 47a67de6a8f..01a16b3606d 100644
--- a/tensorflow/compiler/xla/service/hlo_cost_analysis.cc
+++ b/tensorflow/compiler/xla/service/hlo_cost_analysis.cc
@@ -317,6 +317,11 @@ Status HloCostAnalysis::HandleDomain(const HloInstruction* domain) {
   return Status::OK();
 }
 
+Status HloCostAnalysis::HandleAlternatives(const HloInstruction*) {
+  return Status::OK();
+}
+
+
 /* static */
 int64_t HloCostAnalysis::GetDotFlops(const Shape& lhs_shape,
                                      const Shape& result_shape,
diff --git a/tensorflow/compiler/xla/service/hlo_cost_analysis.h b/tensorflow/compiler/xla/service/hlo_cost_analysis.h
index b849fc3d8df..24df56b6692 100644
--- a/tensorflow/compiler/xla/service/hlo_cost_analysis.h
+++ b/tensorflow/compiler/xla/service/hlo_cost_analysis.h
@@ -168,6 +168,7 @@ class HloCostAnalysis : public ConstDfsHloVisitor {
   Status HandleScatter(const HloInstruction* scatter) override;
   Status HandleGetDimensionSize(const HloInstruction* get_size) override;
   Status HandleSetDimensionSize(const HloInstruction* set_size) override;
+  Status HandleAlternatives(const HloInstruction* alternatives) override;
   Status FinishVisit(const HloInstruction* root) override;
 
   Status Preprocess(const HloInstruction* hlo) override;
diff --git a/tensorflow/compiler/xla/service/hlo_cse.cc b/tensorflow/compiler/xla/service/hlo_cse.cc
index 51981ddd5f2..275eca9f7c9 100644
--- a/tensorflow/compiler/xla/service/hlo_cse.cc
+++ b/tensorflow/compiler/xla/service/hlo_cse.cc
@@ -267,6 +267,7 @@ StatusOr<bool> HloCSE::Run(HloModule* module) {
       }
     }
   }
+  module->RemoveDuplicateTupleOps();
   return changed;
 }
 
diff --git a/tensorflow/compiler/xla/service/hlo_dce.cc b/tensorflow/compiler/xla/service/hlo_dce.cc
index b637ab0878e..45e70dfbe5c 100644
--- a/tensorflow/compiler/xla/service/hlo_dce.cc
+++ b/tensorflow/compiler/xla/service/hlo_dce.cc
@@ -63,6 +63,9 @@ namespace xla {
         computation->RemoveInstructionAndUnusedOperands(dead_root));
     changed = true;
   }
+  changed |= computation->RemoveUnusedTupleOps();
+  computation->Prune();
+
   if (changed) {
     VLOG(3) << "After dce:";
     XLA_VLOG_LINES(3, computation->ToString());
diff --git a/tensorflow/compiler/xla/service/hlo_graph_dumper.cc b/tensorflow/compiler/xla/service/hlo_graph_dumper.cc
index a3015d28913..495ef335c0c 100644
--- a/tensorflow/compiler/xla/service/hlo_graph_dumper.cc
+++ b/tensorflow/compiler/xla/service/hlo_graph_dumper.cc
@@ -1152,6 +1152,8 @@ ColorScheme HloDotDumper::GetInstructionColor(const HloInstruction* instr) {
     case HloOpcode::kCustomCall:
     case HloOpcode::kWhile:
       return kDarkGreen;
+    case HloOpcode::kAlternatives:
+      return kDarkBlue;
   }
 }
 
diff --git a/tensorflow/compiler/xla/service/hlo_instruction.cc b/tensorflow/compiler/xla/service/hlo_instruction.cc
index 7fbf934817e..b847b43385d 100644
--- a/tensorflow/compiler/xla/service/hlo_instruction.cc
+++ b/tensorflow/compiler/xla/service/hlo_instruction.cc
@@ -1139,6 +1139,11 @@ HloInstruction::CreateRngBitGenerator(const Shape& shape, HloInstruction* state,
   return CreateNary(shape, opcode, operands);
 }
 
+/* static */ std::unique_ptr<HloInstruction> HloInstruction::CreateAlternatives(
+    const Shape& shape, absl::Span<HloInstruction* const> operands) {
+  return absl::make_unique<HloAlternatives>(shape, operands);
+}
+
 /* static */ std::unique_ptr<HloInstruction> HloInstruction::CreateMap(
     const Shape& shape, absl::Span<HloInstruction* const> operands,
     HloComputation* map_computation) {
@@ -2000,6 +2005,7 @@ std::unique_ptr<HloInstruction> HloInstruction::CloneWithNewOperands(
     case HloOpcode::kSetDimensionSize:
     case HloOpcode::kTriangularSolve:
     case HloOpcode::kCholesky:
+    case HloOpcode::kAlternatives:
       clone = CloneWithNewOperandsImpl(shape, new_operands, context);
       break;
     // Unary ops.
@@ -2364,6 +2370,10 @@ void HloInstruction::AppendOperand(HloInstruction* operand) {
 
 void HloInstruction::RemoveOperandsAtAscendingIndices(
     absl::Span<const int> ascending_indices) {
+  if (dry_) {
+    return;
+  }
+
   if (ascending_indices.empty()) {
     return;
   }
@@ -2559,6 +2569,12 @@ bool HloInstruction::IdenticalSlowPath(
 }
 
 void HloInstruction::RemoveUser(HloInstruction* user) {
+  // If it is touching the old graph, stop it
+  // else go ahead
+  if (dry_ && user->dry()) {
+    return;
+  }
+
   auto map_it = user_map_.find(user);
   CHECK(map_it != user_map_.end());
 
@@ -2590,6 +2606,11 @@ Status HloInstruction::ReplaceUseWithDifferentShape(
   VLOG(3) << "Replacing uses of " << name() << " in " << user->name()
           << " with " << new_producer->name();
 
+  if (dry_) {
+    parent_->RecordAlternatives(this, new_producer);
+    return Status::OK();
+  }
+
   RemoveUser(user);
 
   TF_RET_CHECK(absl::c_count(user->operands_, this) >= 0);
@@ -2623,6 +2644,11 @@ Status HloInstruction::ReplaceOperandWithDifferentShape(
     return Status::OK();
   }
 
+  if (dry_) {
+    parent_->RecordAlternatives(old_operand, new_operand);
+    return Status::OK();
+  }
+
   operands_[operand_num] = new_operand;
 
   VLOG(3) << "Replacing operand " << operand_num << " of " << name() << " with "
@@ -2666,6 +2692,12 @@ Status HloInstruction::ReplaceAllUsesWith(HloInstruction* new_producer) {
 Status HloInstruction::ReplaceAllUsesWithDifferentShape(
     HloInstruction* new_producer) {
   bool new_producer_is_user = false;
+
+  if (dry_) {
+    parent_->RecordAlternatives(this, new_producer);
+    return Status::OK();
+  }
+
   for (HloInstruction* user : users()) {
     if (user == new_producer) {
       // It's possible that new_producer is a user of this instruction as might
@@ -3629,6 +3661,8 @@ Status HloInstruction::Visit(DfsHloVisitorBase<HloInstructionPtr>* visitor) {
       return visitor->HandleCholesky(this);
     case HloOpcode::kOptimizationBarrier:
       return visitor->HandleOptimizationBarrier(this);
+    case HloOpcode::kAlternatives:
+      return visitor->HandleAlternatives(this);
 
     // These opcodes are not handled here.
     case HloOpcode::kTrace:
diff --git a/tensorflow/compiler/xla/service/hlo_instruction.h b/tensorflow/compiler/xla/service/hlo_instruction.h
index 6c5555dda69..49bfad03303 100644
--- a/tensorflow/compiler/xla/service/hlo_instruction.h
+++ b/tensorflow/compiler/xla/service/hlo_instruction.h
@@ -603,6 +603,9 @@ class HloInstruction {
       const Shape& shape, HloOpcode opcode,
       absl::Span<HloInstruction* const> operands);
 
+  static std::unique_ptr<HloInstruction> CreateAlternatives(
+      const Shape& shape, absl::Span<HloInstruction* const> operands);
+
   // Creates a map instruction, where the computation (given by the handle) is
   // applied element-wise to every element in operands (across the operands,
   // at a given index)
@@ -1206,6 +1209,11 @@ class HloInstruction {
   // Returns the number of users of this instruction.
   int64_t user_count() const { return users_.size(); }
 
+  // Removes all instructions from the user set
+  void clear_users() {
+    users_.clear();
+  }
+
   // Returns the users of this instruction.
   const std::vector<HloInstruction*>& users() const { return users_; }
 
@@ -1510,6 +1518,9 @@ class HloInstruction {
 
   bool IsCustomCall(absl::string_view target) const;
 
+  bool dry() const { return dry_; }
+  void set_dry(bool value) { dry_ = value; }
+
   // Returns the sharding applied to this operator.
   // REQUIRES: has_sharding() is true.
   const HloSharding& sharding() const {
@@ -2109,6 +2120,9 @@ class HloInstruction {
   void RemoveAllOperands() { operands_.clear(); }
 
   void RemoveOperandAt(int index) {
+    if (dry_) {
+      return;
+    }
     operands_.erase(operands_.begin() + index);
   }
 
@@ -2323,6 +2337,7 @@ class HloInstruction {
   // Intrusive flag used by HloComputation, whether this instruction has
   // been marked as dead.
   bool marked_as_dead_;
+  bool dry_ = false;
 
   HloInstruction(const HloInstruction&) = delete;
   HloInstruction& operator=(const HloInstruction&) = delete;
diff --git a/tensorflow/compiler/xla/service/hlo_instructions.cc b/tensorflow/compiler/xla/service/hlo_instructions.cc
index e60dc6985f1..af9f9515148 100644
--- a/tensorflow/compiler/xla/service/hlo_instructions.cc
+++ b/tensorflow/compiler/xla/service/hlo_instructions.cc
@@ -1833,7 +1833,12 @@ HloInstruction* HloFusionInstruction::CloneAndFuseInternal(
       // fusion node. In this case, we don't need to clone
       // 'instruction_to_fuse'.
       CHECK(!in_operand_list);
-      clone = instruction_to_fuse;
+      if (parent()->dry()) {  // if dry mode, make a copy
+        clone = fused_instructions_computation()->AddInstruction(
+            instruction_to_fuse->Clone(/*suffix=*/""));
+      } else {
+        clone = instruction_to_fuse;
+      }
     } else {
       clone = fused_instructions_computation()->AddInstruction(
           instruction_to_fuse->Clone(/*suffix=*/""));
@@ -1935,10 +1940,12 @@ HloInstruction* HloFusionInstruction::CloneAndFuseInternal(
     }
     int64_t index = tuple_elements.size();
     if (instruction_to_fuse->opcode() == HloOpcode::kTuple) {
-      CHECK_EQ(clone, instruction_to_fuse);
+      if (!parent()->dry()) {  // only check when it is not dry mode
+        CHECK_EQ(clone, instruction_to_fuse);
+      }
       index -= clone->operand_count();
       std::vector<HloInstruction*> to_be_removed;
-      const auto& users = clone->users();
+      const auto& users = instruction_to_fuse->users();
       to_be_removed.reserve(users.size());
       for (auto old_gte : users) {
         CHECK_EQ(old_gte->opcode(), HloOpcode::kGetTupleElement);
@@ -2016,6 +2023,8 @@ Status HloFusionInstruction::DeduplicateFusionOperands() {
   if (operands_to_remove.empty()) {
     return Status::OK();
   }
+  // TODO(ohcy): Patch RemoveUnusedParameters to also handle operands, and then
+  // patch this function to account for operands removed in RemoveUnusedParams
   TF_RETURN_IF_ERROR(fused_instructions_computation()
                          ->RemoveUnusedParametersFromFusedComputation());
   RemoveOperandsAtAscendingIndices(operands_to_remove);
@@ -3403,4 +3412,28 @@ HloRngBitGeneratorInstruction::CloneWithNewOperandsImpl(
       shape, new_operands[0], algorithm());
 }
 
+HloAlternatives::HloAlternatives(const Shape& shape,
+                                 absl::Span<HloInstruction* const> operands)
+    : HloInstruction(HloOpcode::kAlternatives, shape) {
+  for (auto operand : operands) {
+    AppendOperand(operand);
+  }
+}
+
+void HloAlternatives::Select(int alternative_index) {
+  CHECK_LT(alternative_index, operand_count());
+  HloInstruction* replace = mutable_operand(alternative_index);
+  std::vector<HloInstruction*> alt_users = users();
+  ReplaceUsesWith(alt_users, replace);
+  if (parent()->root_instruction() == this) {
+    parent()->set_root_instruction(replace);
+  }
+}
+
+std::unique_ptr<HloInstruction> HloAlternatives::CloneWithNewOperandsImpl(
+    const Shape& shape, absl::Span<HloInstruction* const> new_operands,
+    HloCloneContext* /*context*/) const {
+  return absl::make_unique<HloAlternatives>(shape, new_operands);
+}
+
 }  // namespace xla
diff --git a/tensorflow/compiler/xla/service/hlo_instructions.h b/tensorflow/compiler/xla/service/hlo_instructions.h
index 38f2b90adb8..38fffde0e01 100644
--- a/tensorflow/compiler/xla/service/hlo_instructions.h
+++ b/tensorflow/compiler/xla/service/hlo_instructions.h
@@ -2055,6 +2055,17 @@ class HloRngBitGeneratorInstruction : public HloInstruction {
   RandomAlgorithm algorithm_;
 };
 
+class HloAlternatives : public HloInstruction {
+ public:
+  HloAlternatives(const Shape& shape,
+                  absl::Span<HloInstruction* const> operands);
+  void Select(int alternative_index);
+
+  std::unique_ptr<HloInstruction> CloneWithNewOperandsImpl(
+      const Shape& shape, absl::Span<HloInstruction* const> new_operands,
+      HloCloneContext* context) const override;
+};
+
 }  // namespace xla
 
 #endif  // TENSORFLOW_COMPILER_XLA_SERVICE_HLO_INSTRUCTIONS_H_
diff --git a/tensorflow/compiler/xla/service/hlo_module.cc b/tensorflow/compiler/xla/service/hlo_module.cc
index ec800ff7b1e..03658189fd6 100644
--- a/tensorflow/compiler/xla/service/hlo_module.cc
+++ b/tensorflow/compiler/xla/service/hlo_module.cc
@@ -796,6 +796,15 @@ std::vector<HloComputation*> HloModule::MakeNonfusionComputationsSorted()
   return result;
 }
 
+std::vector<HloComputation*> HloModule::MakeFusionComputations() const {
+  std::vector<HloComputation*> result = MakeComputationPostOrder();
+  result.erase(std::remove_if(
+                   result.begin(), result.end(),
+                   [](HloComputation* c) { return !c->IsFusionComputation(); }),
+               result.end());
+  return result;
+}
+
 std::unique_ptr<HloModule> HloModule::Clone(const std::string& suffix) const {
   return Clone(config(), suffix);
 }
@@ -886,6 +895,33 @@ HloComputation* HloModule::GetComputationWithName(absl::string_view name) {
   return it == computations_in_module.end() ? nullptr : *it;
 }
 
+uint64_t HloModule::CalledComputationHash() const {
+  return absl::HashOf(*this);
+}
+
+void HloModule::Prune() {
+  auto computations = MakeComputationPostOrder();
+  for (auto* computation : computations) {
+    computation->Prune();
+  }
+  RemoveUnusedComputations();
+}
+
+void HloModule::SetDry(bool dry_mode) {
+  if (dry_mode != dry_mode_) {
+    for (xla::HloComputation* computation : MakeNonfusionComputations()) {
+      computation->set_dry(dry_mode);
+    }
+    dry_mode_ = dry_mode;
+    // Remove unused computations that might been orphaned during the
+    // set dry_off processes (e.g. alternatives deleted due to generating
+    // a cycle)
+    if (!dry_mode_) {
+      RemoveUnusedComputations();
+    }
+  }
+}
+
 /* static */ std::atomic<int> HloModule::next_unique_module_id_(0);
 
 }  // namespace xla
diff --git a/tensorflow/compiler/xla/service/hlo_module.h b/tensorflow/compiler/xla/service/hlo_module.h
index a20f6cd383c..6ff756a5645 100644
--- a/tensorflow/compiler/xla/service/hlo_module.h
+++ b/tensorflow/compiler/xla/service/hlo_module.h
@@ -167,6 +167,10 @@ class HloModule {
     return H::combine(std::move(h), computations.size());
   }
 
+  // Generates a hash value of an HLO module. Focuses on
+  // the call graphs between multiple computations.
+  uint64_t CalledComputationHash() const;
+
   // Gets the computations in this module.
   //
   // Returns a view of HloComputation*s, so you can iterate over this in the
@@ -210,6 +214,10 @@ class HloModule {
     }
   }
 
+  void Prune();
+
+  void SetDry(bool dry_mode);
+
   // Compute and return a post order of all computations in the module. The sort
   // is defined like so: if computation A has an instruction which calls
   // computation B, then A will appear after B in the sort.
@@ -235,6 +243,8 @@ class HloModule {
   // MakeNonfusionComputations().
   std::vector<HloComputation*> MakeNonfusionComputations() const;
 
+  std::vector<HloComputation*> MakeFusionComputations() const;
+
   // Same as MakeNonfusionComputations() but sorting computations by content.
   std::vector<HloComputation*> MakeNonfusionComputationsSorted() const;
 
@@ -355,6 +365,20 @@ class HloModule {
 
   Status CheckUniqueNamesAndIdsForComputationsAndInstructions() const;
 
+  // Dedup duplicate operands in Tuple instructions
+  void RemoveUnusedTupleOps() {
+    for (HloComputation* computation : computations()) {
+      computation->RemoveUnusedTupleOps();
+    }
+  }
+
+  // Dedup duplicate operands in Tuple instructions
+  void RemoveDuplicateTupleOps() {
+    for (HloComputation* computation : computations()) {
+      computation->RemoveDuplicateTupleOps();
+    }
+  }
+
   // Checks if this config has a list of entry parameters' HLO shardings for
   // SPMD.
   bool has_spmd_parameters_shardings() const {
@@ -427,6 +451,8 @@ class HloModule {
     return profile_info_list_;
   }
 
+  bool dry_mode() const { return dry_mode_; }
+
   void set_relative_speedup(double relative_speedup) {
     relative_speedup_ = relative_speedup;
   }
@@ -508,6 +534,9 @@ class HloModule {
 
   // The unoptimized module fingerprint.
   std::string autofdo_fingerprint_;
+
+  // Whether or not the module is currently in dry_mode
+  bool dry_mode_ = false;;
 };
 
 }  // namespace xla
diff --git a/tensorflow/compiler/xla/service/hlo_opcode.h b/tensorflow/compiler/xla/service/hlo_opcode.h
index e58e6804e44..fb2c8777ef3 100644
--- a/tensorflow/compiler/xla/service/hlo_opcode.h
+++ b/tensorflow/compiler/xla/service/hlo_opcode.h
@@ -59,6 +59,7 @@ namespace xla {
   V(kAsyncUpdate, "async-update", 1)                                           \
   V(kAsyncDone, "async-done", 1)                                               \
   V(kAtan2, "atan2", 2)                                                        \
+  V(kAlternatives, "alternatives", kHloOpcodeIsVariadic)                       \
   V(kBatchNormGrad, "batch-norm-grad", 5)                                      \
   V(kBatchNormInference, "batch-norm-inference", 5)                            \
   V(kBatchNormTraining, "batch-norm-training", 3)                              \
diff --git a/tensorflow/compiler/xla/service/hlo_parser.cc b/tensorflow/compiler/xla/service/hlo_parser.cc
index 4b93414d347..75ac81985dd 100644
--- a/tensorflow/compiler/xla/service/hlo_parser.cc
+++ b/tensorflow/compiler/xla/service/hlo_parser.cc
@@ -87,6 +87,7 @@ bool CanInferShape(HloOpcode code) {
     case HloOpcode::kAdd:
     case HloOpcode::kAddDependency:
     case HloOpcode::kAfterAll:
+    case HloOpcode::kAlternatives:
     case HloOpcode::kAtan2:
     case HloOpcode::kBatchNormGrad:
     case HloOpcode::kBatchNormInference:
@@ -1339,6 +1340,23 @@ HloInstruction* HloParserImpl::CreateInstruction(  // NOLINT
           *shape, opcode, operands[0], operands[1], operands[2]));
     }
     // Other supported ops.
+    case HloOpcode::kAlternatives: {
+      if (!preset_operands && !ParseOperands(&operands, builder)) {
+        return nullptr;
+      }
+      if (!maybe_infer_shape([&] {
+            absl::InlinedVector<const Shape*, 2> arg_shapes;
+            arg_shapes.reserve(operands.size());
+            for (auto* operand : operands) {
+              arg_shapes.push_back(&operand->shape());
+            }
+            return ShapeInference::InferVariadicOpShape(opcode, arg_shapes);
+          })) {
+        return nullptr;
+      }
+      return builder->AddInstruction(
+          HloInstruction::CreateAlternatives(*shape, operands));
+    }
     case HloOpcode::kConvert: {
       if ((!preset_operands &&
            !ParseOperands(&operands, builder, /*expected_size=*/1)) ||
diff --git a/tensorflow/compiler/xla/service/hlo_pass_pipeline.cc b/tensorflow/compiler/xla/service/hlo_pass_pipeline.cc
index 6a542a67208..aaf80ebe109 100644
--- a/tensorflow/compiler/xla/service/hlo_pass_pipeline.cc
+++ b/tensorflow/compiler/xla/service/hlo_pass_pipeline.cc
@@ -15,6 +15,7 @@ limitations under the License.
 
 #include "tensorflow/compiler/xla/service/hlo_pass_pipeline.h"
 
+#include <fstream>
 #include <functional>
 #include <string>
 
@@ -30,6 +31,7 @@ limitations under the License.
 #include "tensorflow/compiler/xla/util.h"
 #include "tensorflow/core/platform/errors.h"
 #include "tensorflow/core/platform/logging.h"
+#include "tensorflow/core/util/env_var.h"
 
 namespace xla {
 
@@ -287,4 +289,14 @@ StatusOr<bool> HloPassPipeline::RunOnModuleGroup(HloModuleGroup* module_group) {
                            module_group->module(0).config().debug_options());
 }
 
+std::set<std::string> HloPassPipeline::ExtractDrySandwichSetFromEnv() {
+  std::vector<std::string> dry_passes;
+  TF_CHECK_OK(tensorflow::ReadStringsFromEnvVar(
+      /*env_var_name=*/"DRY", /*default_val=*/"", &dry_passes));
+  return std::set<std::string>(dry_passes.begin(), dry_passes.end());
+}
+
+std::set<std::string> HloPassPipeline::dry_sandwich_set =
+    ExtractDrySandwichSetFromEnv();
+
 }  // namespace xla
diff --git a/tensorflow/compiler/xla/service/hlo_pass_pipeline.h b/tensorflow/compiler/xla/service/hlo_pass_pipeline.h
index 73b89c5d492..b2593163984 100644
--- a/tensorflow/compiler/xla/service/hlo_pass_pipeline.h
+++ b/tensorflow/compiler/xla/service/hlo_pass_pipeline.h
@@ -18,13 +18,16 @@ limitations under the License.
 
 #include <algorithm>
 #include <memory>
+#include <set>
 #include <string>
 #include <vector>
 
 #include "absl/memory/memory.h"
 #include "absl/strings/str_cat.h"
 #include "tensorflow/compiler/xla/service/compilation_stats.h"
+#include "tensorflow/compiler/xla/service/dry_mode.h"
 #include "tensorflow/compiler/xla/service/hlo_module.h"
+#include "tensorflow/compiler/xla/service/hlo_pass_fix.h"
 #include "tensorflow/compiler/xla/service/hlo_pass_interface.h"
 #include "tensorflow/compiler/xla/statusor.h"
 #include "tensorflow/compiler/xla/types.h"
@@ -33,6 +36,12 @@ namespace xla {
 
 class PhaseOrderPipeline;
 
+template <typename T>
+struct is_hlo_pass_fix : std::false_type {};
+
+template <typename T, int S>
+struct is_hlo_pass_fix<HloPassFix<T, S>> : std::true_type {};
+
 // Pipeline of HLO passes.
 class HloPassPipeline : public HloPassInterface {
  public:
@@ -45,6 +54,36 @@ class HloPassPipeline : public HloPassInterface {
     }
   }
   absl::string_view name() const override { return name_; }
+  static std::set<std::string> dry_sandwich_set;
+
+  // The helpers are used to detect HloPassFix, as the Fix passes will
+  // repeat internally, we can't sandwich it.
+  // TODO: find a solution for this, I think probably we need to remove the fix
+  // passes and iterate externally.
+  template <typename T, typename... Args>
+  T& AddPassHelper(std::false_type, Args&&... args) {
+    CHECK(!run_called_) << "AddPass cannot be called after Run";
+    auto pass = new T(std::forward<Args>(args)...);
+    std::string pass_name(pass->name());
+    if (dry_sandwich_set.find(pass_name) != dry_sandwich_set.end()) {
+      LOG(ERROR) << "Sandwiching " << pass_name << " with dry mode on/off.";
+      passes_.push_back(std::make_unique<DryModeOn>());
+    }
+    passes_.push_back(std::unique_ptr<T>(pass));
+    if (dry_sandwich_set.find(pass_name) != dry_sandwich_set.end()) {
+      passes_.push_back(std::make_unique<DryModeOff>());
+    }
+    return *pass;
+  }
+
+  template <typename T, typename... Args>
+  T& AddPassHelper(std::true_type, Args&&... args) {
+    CHECK(!run_called_) << "AddPass cannot be called after Run";
+    auto pass = new T(std::forward<Args>(args)...);
+    std::string pass_name(pass->name());
+    passes_.push_back(std::unique_ptr<T>(pass));
+    return *pass;
+  }
 
   // Add a pass to the pipeline. It should be called with the arguments for the
   // pass constructor:
@@ -54,10 +93,8 @@ class HloPassPipeline : public HloPassInterface {
   // Returns a reference to the added pass.
   template <typename T, typename... Args>
   T& AddPass(Args&&... args) {
-    CHECK(!run_called_) << "AddPass cannot be called after Run";
-    auto pass = new T(std::forward<Args>(args)...);
-    passes_.push_back(std::unique_ptr<T>(pass));
-    return *pass;
+    return AddPassHelper<T, Args...>(typename is_hlo_pass_fix<T>::type(),
+                                     std::forward<Args>(args)...);
   }
 
   // Add an invariant-checking pass to the pipeline. It will be run before and
@@ -89,6 +126,8 @@ class HloPassPipeline : public HloPassInterface {
   // Return reference to pass specified by index.
   HloPassInterface& GetPass(int index) { return *passes_[index]; }
 
+  static std::set<std::string> ExtractDrySandwichSetFromEnv();
+
  private:
   // Returns the set of passes which are enabled. DebugOptions can selectively
   // disable passes via --xla_disable_hlo_passes flag.
diff --git a/tensorflow/compiler/xla/service/hlo_verifier.cc b/tensorflow/compiler/xla/service/hlo_verifier.cc
index b82223ca455..6c27d600a61 100644
--- a/tensorflow/compiler/xla/service/hlo_verifier.cc
+++ b/tensorflow/compiler/xla/service/hlo_verifier.cc
@@ -1552,6 +1552,10 @@ Status ShapeVerifier::HandleSetDimensionSize(HloInstruction* set_size) {
                         set_size->operand(1)->shape(), set_size->dimension()));
 }
 
+Status ShapeVerifier::HandleAlternatives(HloInstruction* alternatives) {
+  return Status::OK();
+}
+
 Status ShapeVerifier::CheckShape(const HloInstruction* instruction,
                                  const Shape& inferred_shape,
                                  bool only_compare_minor_to_major_in_layout) {
diff --git a/tensorflow/compiler/xla/service/hlo_verifier.h b/tensorflow/compiler/xla/service/hlo_verifier.h
index 998b70ec380..4513849d5de 100644
--- a/tensorflow/compiler/xla/service/hlo_verifier.h
+++ b/tensorflow/compiler/xla/service/hlo_verifier.h
@@ -119,6 +119,7 @@ class ShapeVerifier : public DfsHloVisitor {
   Status HandleGetDimensionSize(HloInstruction* get_size) override;
   Status HandleSetDimensionSize(HloInstruction* set_size) override;
   Status HandleAddDependency(HloInstruction* add_dependency) override;
+  Status HandleAlternatives(HloInstruction* alternatives) override;
 
   Status FinishVisit(HloInstruction*) override { return Status::OK(); }
 
diff --git a/tensorflow/compiler/xla/service/instruction_fusion.cc b/tensorflow/compiler/xla/service/instruction_fusion.cc
index 1ae798fc4e9..33ac914a0cc 100644
--- a/tensorflow/compiler/xla/service/instruction_fusion.cc
+++ b/tensorflow/compiler/xla/service/instruction_fusion.cc
@@ -459,11 +459,15 @@ class ReversePostOrderFusionQueue : public FusionQueue {
   void OnFusingInstruction(HloInstruction* fusion,
                            HloInstruction* original_producer,
                            HloInstruction* original_consumer) override {
-    // Fusing an instruction into a fusion instruction can change the operand
-    // set of the fusion instruction. For simplicity just re-enqueue the
-    // instruction and reconsider it for further fusion in the next iteration.
-    InsertOrDie(&post_order_index_, fusion, post_order_.size());
-    post_order_.push_back(fusion);
+    // we turn dry run on for fusion, so that it won't be modified
+    if (fusion->parent()->dry()) {
+      fusion->set_dry(true);
+    } else {
+      // for dry run mode off, we re-insert into queue as usual
+      // so that it won't affect non-modified XLA passes.
+      InsertOrDie(&post_order_index_, fusion, post_order_.size());
+      post_order_.push_back(fusion);
+    }
   }
 
   void RemoveInstruction(HloInstruction* instruction) override {
@@ -707,6 +711,21 @@ HloInstruction* InstructionFusion::AddFusionInstruction(
   HloInstruction* fusion_instruction;
   auto kind = ChooseKind(producer, consumer);
   if (consumer->opcode() == HloOpcode::kFusion) {
+    if (consumer->dry()) {
+      // Don't keep fusing into it, make a copy when consumer turns on
+      // dry run, we need a clone to continue adding.
+      // TODO(linmin): limit the depth of fusion to avoid combinatorial
+      // explosion
+      HloInstruction* consumer_clone =
+          consumer->parent()->AddInstruction(consumer->Clone());
+      // Since this is in dry mode, this will trigger
+      // RecordAlternatives, which will trace down to the most original
+      // instruction that the fusion node is replacing.
+      // TODO(wangyzh): this could have problems when there're multiple outputs
+      // we need to handle this in multi_output_fusion
+      consumer->parent()->ReplaceInstruction(consumer, consumer_clone);
+      consumer = consumer_clone;
+    }
     fusion_instruction = consumer;
     if (kind != fusion_instruction->fusion_kind()) {
       fusion_instruction->set_fusion_kind(kind);
diff --git a/tensorflow/compiler/xla/service/service.cc b/tensorflow/compiler/xla/service/service.cc
index 3ab70c0da3b..288e03e8e69 100644
--- a/tensorflow/compiler/xla/service/service.cc
+++ b/tensorflow/compiler/xla/service/service.cc
@@ -284,6 +284,7 @@ StatusOr<std::unique_ptr<HloModuleConfig>> Service::CreateModuleConfig(
                             aot_options);
 }
 
+// MARK: compilation call chain
 StatusOr<std::vector<std::unique_ptr<Executable>>> Service::BuildExecutables(
     const std::vector<const HloModuleProto*>& module_protos,
     std::vector<std::unique_ptr<HloModuleConfig>> module_configs,
@@ -779,6 +780,7 @@ Status Service::GetDeviceHandles(const GetDeviceHandlesRequest* arg,
   return Status::OK();
 }
 
+// MARK: compilation call chain
 StatusOr<std::unique_ptr<Executable>> Service::BuildExecutable(
     const HloModuleProto& module_proto,
     std::unique_ptr<HloModuleConfig> module_config, Backend* backend,
@@ -793,6 +795,7 @@ StatusOr<std::unique_ptr<Executable>> Service::BuildExecutable(
       CreateModuleFromProto(module_proto, *module_config, run_backend_only));
   DumpHloModuleIfEnabled(*module, kBeforeOptimizationsDumpName);
 
+  // MARK: compilation call chain
   if (!run_backend_only) {
     TF_ASSIGN_OR_RETURN(module, backend->compiler()->RunHloPasses(
                                     std::move(module), executor, options));
diff --git a/tensorflow/compiler/xla/shape.h b/tensorflow/compiler/xla/shape.h
index e2c3089b445..abd42e5331f 100644
--- a/tensorflow/compiler/xla/shape.h
+++ b/tensorflow/compiler/xla/shape.h
@@ -143,6 +143,29 @@ class Shape {
   void clear_tuple_shapes() { tuple_shapes_.clear(); }
   const std::vector<Shape>& tuple_shapes() const { return tuple_shapes_; }
   std::vector<Shape>* mutable_tuple_shapes() { return &tuple_shapes_; }
+  void remove_tuple_shapes_at_ascending_indices(
+      absl::Span<const int> ascending_indices) {
+    if (ascending_indices.empty()) {
+      return;
+    }
+    int next_index = 0;
+    int removed_count = 0;
+    for (int to_remove : ascending_indices) {
+      while (next_index < to_remove) {
+        tuple_shapes_[next_index - removed_count] = tuple_shapes_[next_index];
+        ++next_index;
+      }
+      CHECK_LT(to_remove, tuple_shapes_.size());
+      ++removed_count;
+      ++next_index;
+    }
+    while (next_index < tuple_shapes_.size()) {
+      tuple_shapes_[next_index - removed_count] = tuple_shapes_[next_index];
+      ++next_index;
+    }
+    CHECK_EQ(removed_count, ascending_indices.size());
+    tuple_shapes_.resize(tuple_shapes_.size() - removed_count);
+  }
 
   // Methods for accessing the layout field.
   bool has_layout() const { return layout_.format() != INVALID_FORMAT; }
diff --git a/tensorflow/compiler/xla/tests/BUILD b/tensorflow/compiler/xla/tests/BUILD
index 8b9738f15c0..12396a6cfea 100644
--- a/tensorflow/compiler/xla/tests/BUILD
+++ b/tensorflow/compiler/xla/tests/BUILD
@@ -151,6 +151,7 @@ cc_library(
     testonly = True,
     srcs = ["hlo_test_base.cc"],
     hdrs = ["hlo_test_base.h"],
+    visibility = ["//visibility:public"],
     deps = [
         ":literal_test_util",
         ":manifest_checking_test",
diff --git a/tensorflow/compiler/xla/tools/BUILD b/tensorflow/compiler/xla/tools/BUILD
index cc358660c72..a9a6add9175 100644
--- a/tensorflow/compiler/xla/tools/BUILD
+++ b/tensorflow/compiler/xla/tools/BUILD
@@ -217,6 +217,7 @@ cc_library(
     name = "hlo_extractor",
     srcs = ["hlo_extractor.cc"],
     hdrs = ["hlo_extractor.h"],
+    visibility = ["//visibility:public"],
     deps = [
         "//tensorflow/compiler/xla:status",
         "//tensorflow/compiler/xla/service:hlo",
@@ -230,6 +231,7 @@ cc_library(
 tf_cc_binary(
     name = "interactive_graphviz",
     srcs = ["interactive_graphviz.cc"],
+    visibility = ["//visibility:public"],
     deps = [
         ":hlo_extractor",
         "//tensorflow/compiler/xla/service:hlo_graph_dumper",
@@ -262,7 +264,7 @@ cc_library(
     name = "hlo_module_loader",
     srcs = ["hlo_module_loader.cc"],
     hdrs = ["hlo_module_loader.h"],
-    visibility = ["//tensorflow/compiler/xla:friends"],
+    visibility = ["//visibility:public"],
     deps = [
         "//tensorflow/compiler/xla:debug_options_flags",
         "//tensorflow/compiler/xla:statusor",
diff --git a/tensorflow/core/BUILD b/tensorflow/core/BUILD
index 93bdd8402ac..255b3d8db43 100644
--- a/tensorflow/core/BUILD
+++ b/tensorflow/core/BUILD
@@ -1299,6 +1299,7 @@ cc_library(
         "@com_google_absl//absl/base:core_headers",
         "//tensorflow/core/platform/default/build_config:platformlib",
     ] + if_static([":lib_internal_impl"]),
+    visibility = ["//visibility:public"],
 )
 
 # Until we can ditch config=monolithic on windows, we have to provide an always
diff --git a/tensorflow/core/util/BUILD b/tensorflow/core/util/BUILD
index 8c267148fb7..1a1a48e9cb2 100644
--- a/tensorflow/core/util/BUILD
+++ b/tensorflow/core/util/BUILD
@@ -596,6 +596,7 @@ cc_library(
     name = "env_var",
     srcs = ["env_var.cc"],
     hdrs = ["env_var.h"],
+    visibility = ["//visibility:public"],
     deps = [
         "//tensorflow/core/platform:errors",
         "//tensorflow/core/platform:logging",
diff --git a/tensorflow/python/BUILD b/tensorflow/python/BUILD
index 3495b50db84..719d9030a3f 100644
--- a/tensorflow/python/BUILD
+++ b/tensorflow/python/BUILD
@@ -4207,6 +4207,7 @@ alias(
 alias(
     name = "pybind11_lib",
     actual = "//tensorflow/python/lib/core:pybind11_lib",
+    visibility = ["//visibility:public"],
 )
 
 alias(
diff --git a/tensorflow/workspace2.bzl b/tensorflow/workspace2.bzl
index d56d8ad9c7f..025db66156b 100644
--- a/tensorflow/workspace2.bzl
+++ b/tensorflow/workspace2.bzl
@@ -835,6 +835,13 @@ def _tf_repositories():
         urls = tf_mirror_urls("https://github.com/nlohmann/json/archive/v3.4.0.tar.gz"),
     )
 
+    tf_http_archive(
+        name = "pybind11_bazel",
+        urls = tf_mirror_urls("https://github.com/pybind/pybind11_bazel/archive/26973c0ff320cb4b39e45bc3e4297b82bc3a6c09.zip"),
+        sha256 = "a5666d950c3344a8b0d3892a88dc6b55c8e0c78764f9294e806d69213c03f19d",
+        strip_prefix = "pybind11_bazel-26973c0ff320cb4b39e45bc3e4297b82bc3a6c09",
+    )
+
     tf_http_archive(
         name = "pybind11",
         urls = tf_mirror_urls("https://github.com/pybind/pybind11/archive/v2.9.0.tar.gz"),
diff --git a/tensorflow/xla_standalone/BUILD b/tensorflow/xla_standalone/BUILD
new file mode 100644
index 00000000000..a97fe46dc7a
--- /dev/null
+++ b/tensorflow/xla_standalone/BUILD
@@ -0,0 +1,71 @@
+load("//tensorflow:tensorflow.bzl", "tf_cc_binary")
+
+package(default_visibility = ["//visibility:public"])
+
+tf_cc_binary(
+    name = "xla_compile",
+    srcs = ["xla_compile.cc"],
+    copts = [
+        "-fexceptions",
+    ],
+    data = [
+        "hlo_test.txt",
+    ],
+    deps = [
+        ":hlo_graph",
+        "//tensorflow/compiler/jit:xla_gpu_jit",
+        "//tensorflow/compiler/xla:literal",
+        "//tensorflow/compiler/xla:literal_util",
+        "//tensorflow/compiler/xla:shape_util",
+        "//tensorflow/compiler/xla:status",
+        "//tensorflow/compiler/xla:statusor",
+        "//tensorflow/compiler/xla/pjrt:cpu_device",
+        "//tensorflow/compiler/xla/pjrt:gpu_device",
+        "//tensorflow/compiler/xla/pjrt:pjrt_client",
+        "//tensorflow/compiler/xla/service:hlo_pass_pipeline",
+        "//tensorflow/compiler/xla/service:hlo_proto_cc",
+        "//tensorflow/compiler/xla/tools:hlo_module_loader",
+        "@com_github_gflags_gflags//:gflags",
+    ],
+)
+
+tf_cc_binary(
+    name = "graphviz",
+    srcs = ["graphviz.cc"],
+    copts = [
+        "-fexceptions",
+    ],
+    deps = [
+        "//tensorflow/compiler/xla/client:client_library",
+        "//tensorflow/compiler/xla/client:local_client",
+        "//tensorflow/compiler/xla/service:compiler",
+        "//tensorflow/compiler/xla/service:cpu_plugin",
+        "//tensorflow/compiler/xla/service:hlo_graph_dumper",
+        "//tensorflow/compiler/xla/service:hlo_proto_cc",
+        "//tensorflow/compiler/xla/service:hlo_runner",
+        "//tensorflow/compiler/xla/service:local_service",
+        "//tensorflow/compiler/xla/tools:hlo_extractor",
+        "//tensorflow/core:framework_internal",
+        "//tensorflow/core:lib",
+        "//tensorflow/core:protos_all_cc",
+        "@com_github_gflags_gflags//:gflags",
+        "@com_google_absl//absl/algorithm:container",
+        "@com_google_absl//absl/container:flat_hash_set",
+        "@com_google_absl//absl/strings",
+    ],
+)
+
+cc_library(
+    name = "hlo_graph",
+    srcs = ["hlo_graph.cc"],
+    hdrs = ["hlo_graph.h"],
+    deps = [
+        "//tensorflow/compiler/xla:types",
+        "//tensorflow/compiler/xla:util",
+        "//tensorflow/compiler/xla/service:hlo",
+        "//tensorflow/core:lib",
+        "//tensorflow/core:lib_internal",
+        "@com_google_absl//absl/base",
+        "@com_google_absl//absl/container:flat_hash_map",
+    ],
+)
diff --git a/tensorflow/xla_standalone/graphviz.cc b/tensorflow/xla_standalone/graphviz.cc
new file mode 100644
index 00000000000..6ab2c1f16da
--- /dev/null
+++ b/tensorflow/xla_standalone/graphviz.cc
@@ -0,0 +1,155 @@
+// Copyright 2022 Garena Online Private Limited.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//
+// Non-interactive version of xla/tools/interactive_graphviz
+#include <gflags/gflags.h>
+#include <stdio.h>
+#include <unistd.h>
+
+#include <functional>
+#include <sstream>
+#include <string>
+#include <utility>
+
+#include "absl/algorithm/container.h"
+#include "absl/container/flat_hash_set.h"
+#include "absl/strings/match.h"
+#include "absl/strings/numbers.h"
+#include "absl/strings/str_cat.h"
+#include "absl/strings/str_join.h"
+#include "absl/strings/str_split.h"
+#include "tensorflow/compiler/xla/client/client_library.h"
+#include "tensorflow/compiler/xla/client/local_client.h"
+#include "tensorflow/compiler/xla/service/compiler.h"
+#include "tensorflow/compiler/xla/service/hlo.pb.h"
+#include "tensorflow/compiler/xla/service/hlo_runner.h"
+#include "tensorflow/compiler/xla/service/local_service.h"
+#include "tensorflow/compiler/xla/tools/hlo_extractor.h"
+#include "tensorflow/core/lib/io/path.h"
+#include "tensorflow/core/platform/init_main.h"
+#include "tensorflow/core/platform/logging.h"
+#include "tensorflow/core/platform/subprocess.h"
+#include "tensorflow/core/protobuf/error_codes.pb.h"
+#include "tensorflow/core/util/command_line_flags.h"
+
+DEFINE_string(hlo, "-", "hlo text file");  // by default read from stdin
+DEFINE_string(html, "-", "the path to the generated html file");
+DEFINE_string(browser, "/usr/bin/sensible-browser",
+              "default browser to view hlo graph.");
+
+namespace xla {
+namespace tools {
+namespace {
+
+void OpenUrl(absl::string_view url) {
+  std::cerr << url << std::endl;
+
+  // If it is a url, try to open it up in the user's browser too.
+  if (absl::StartsWithIgnoreCase(url, "http://") ||
+      absl::StartsWithIgnoreCase(url, "https://") ||
+      absl::StartsWithIgnoreCase(url, "file://")) {
+    const char* browser_bin = FLAGS_browser.empty()
+                                  ? "/usr/bin/sensible-browser"
+                                  : FLAGS_browser.c_str();
+    tensorflow::SubProcess p;
+    p.SetProgram(browser_bin, {browser_bin, std::string(url)});
+    p.Start();
+  } else {
+    std::cerr << "\nExpected a URL, but got strange graph result (dumped "
+                 "above).  If this isn't what you expected, maybe file a bug?"
+              << std::endl;
+  }
+}
+
+void RenderAndDisplayGraph(
+    const std::function<StatusOr<std::string>(RenderedGraphFormat)>& renderer) {
+  StatusOr<std::string> url_result = renderer(RenderedGraphFormat::kUrl);
+  if (url_result.ok()) {
+    std::string url = url_result.ValueOrDie();
+    OpenUrl(url);
+    return;
+  }
+
+  // Ignore UNAVAILABLE errors; these are expected when there's no URL renderer
+  // plugin registered.
+  if (url_result.status().code() != tensorflow::error::UNAVAILABLE) {
+    std::cerr << "Unable to render graph as URL: " << url_result.status()
+              << std::endl;
+    std::cerr << "Trying as HTML..." << std::endl;
+  }
+
+  auto* env = tensorflow::Env::Default();
+  StatusOr<std::string> html_result = renderer(RenderedGraphFormat::kHtml);
+  if (!html_result.ok()) {
+    std::cerr << "Failed to render graph as HTML: " << html_result.status()
+              << std::endl;
+    return;
+  }
+
+  if (FLAGS_html == "-") {
+    std::cout << html_result.ValueOrDie() << std::endl;
+  } else {
+    std::string temp_file_path = tensorflow::io::JoinPath(
+        ".", absl::StrFormat("%s.%d.html", FLAGS_html, env->NowMicros()));
+    auto status = tensorflow::WriteStringToFile(
+        env, FLAGS_html, std::move(html_result).ValueOrDie());
+    if (status.ok()) {
+      OpenUrl(absl::StrCat("file://", temp_file_path));
+      return;
+    }
+    std::cerr << "Failed to write rendered HTML graph to " << temp_file_path
+              << ": " << status;
+  }
+  // We don't bother trying kDot, because kHTML should always work (or if it
+  // doesn't, we don't have any reason to believe kDot will work better).
+}
+
+void GraphViz(HloModule* module) {
+  HloRenderOptions hlo_render_options;
+  hlo_render_options.show_backend_config = true;
+  hlo_render_options.show_fusion_subcomputations = true;
+
+  const HloComputation* comp = module->entry_computation();
+  RenderAndDisplayGraph([&](RenderedGraphFormat format) {
+    return xla::RenderGraph(
+        *comp, /*label=*/"", comp->parent()->config().debug_options(), format,
+        /*hlo_execution_profile=*/nullptr, hlo_render_options);
+  });
+}
+
+}  // namespace
+}  // namespace tools
+}  // namespace xla
+
+int main(int argc, char** argv) {
+  tensorflow::port::InitMain(argv[0], &argc, &argv);
+  gflags::ParseCommandLineFlags(&argc, &argv, true);
+  std::unique_ptr<xla::HloModule> module;
+  if (FLAGS_hlo == "-") {
+    std::stringstream ss;
+    std::string s;
+    while (std::getline(std::cin, s)) {
+      ss << s << "\n";
+    }
+    module = xla::HloRunner::CreateModuleFromString(
+                 ss.str(), xla::GetDebugOptionsFromFlags())
+                 .ValueOrDie();
+  } else {
+    module = xla::HloRunner::ReadModuleFromHloTextFile(
+                 FLAGS_hlo, xla::GetDebugOptionsFromFlags())
+                 .ValueOrDie();
+  }
+  xla::tools::GraphViz(module.get());
+  return 0;
+}
diff --git a/tensorflow/xla_standalone/hlo_graph.cc b/tensorflow/xla_standalone/hlo_graph.cc
new file mode 100644
index 00000000000..75d2134b129
--- /dev/null
+++ b/tensorflow/xla_standalone/hlo_graph.cc
@@ -0,0 +1,360 @@
+// Copyright 2022 Garena Online Private Limited.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#include "tensorflow/xla_standalone/hlo_graph.h"
+
+#include <string>
+
+#include "absl/base/casts.h"
+#include "tensorflow/compiler/xla/map_util.h"
+#include "tensorflow/compiler/xla/service/hlo_opcode.h"
+#include "tensorflow/compiler/xla/types.h"
+#include "tensorflow/core/lib/core/status.h"
+#include "tensorflow/core/platform/types.h"
+
+namespace xla {
+
+HloGraph::HloGraph(const HloModule* m)
+    : parent_hlo_module_(const_cast<HloModule*>(m)),
+      uid_(m->unique_id()),
+      name_(m->name()) {
+  Build(m);
+}
+
+void HloGraph::Clear() {
+  node_feats.Clear();
+  in_edge_feats.Clear();
+  out_edge_feats.Clear();
+  user_list_offsets.clear();
+  operand_list_offsets.clear();
+  user_list_indices.clear();
+  operand_list_indices.clear();
+  alternative_indices_.clear();
+  uid_to_node_ind_.clear();
+  uid_to_inst_.clear();
+}
+
+bool HloGraph::Build(const HloModule* m) {
+  int gid = -1;
+  int node_ind = 0;
+  Clear();
+
+  std::vector<HloInstruction*> inst_list;
+
+  // Since CSR/CSC is a flatten linear structure where each item's
+  // global index needs to be decided first before values being
+  // filled up. We need to first compute the sizes/offsets, then
+  // put indices into the neighbor list array (xxx_indices).
+  for (auto c : m->MakeComputationPostOrder()) {
+    gid++;
+    for (auto inst : c->MakeInstructionPostOrder()) {
+      inst_list.push_back(inst);
+      int uid = inst->unique_id();
+      std::string name = inst->name();
+
+      // Add to node_feats
+      node_feats.uids.push_back(uid);
+      node_feats.names.push_back(name);
+      node_feats.gids.push_back(gid);
+
+      // Add to offsets
+      user_list_offsets.push_back(inst->user_count());
+      if (inst->called_computations().empty()) {
+        operand_list_offsets.push_back(inst->operand_count());
+      } else {
+        // if instruction calls computation(s), it should have
+        // no operand other than that computation's root instruction.
+        // current operands will all point to that computation's
+        // params.
+        operand_list_offsets.push_back(0);
+      }
+
+      // add uid to node id hash map
+      // when we build neighbor list we will need these indices.
+      uid_to_node_ind_.insert({uid, node_ind++});
+      uid_to_inst_.insert({uid, inst});
+    }
+  }
+
+  // For instruction that calls computations, we:
+  // 1) add instruction as its computation's root inst's extra user
+  // 2) connect instruction's operands to its called computation's params in
+  // order
+  //    2.1) for instruction's each operand, add one user
+  //    2.2) for each called computation's param, add one operand
+
+  // Update user/operand count
+  for (auto inst : inst_list) {
+    int uid = inst->unique_id();
+    auto called_comps = inst->called_computations();
+    if (called_comps.size() > 0) {
+      // for kCall and kFusion, called_comps.size() should be 1
+      // for kWhile and kCondition, called_comps.size() should be 2
+      // and body_func/cond_func in kWhile and cond_true/cond_false func
+      // in kCondition share the same set of param.
+      // So it's safe to apply operand of current instruction to
+      // both computations's params.
+
+      // add one operand count to current instruction (from called comp).
+      operand_list_offsets[uid_to_node_ind_[uid]]++;
+      for (auto cc : called_comps) {
+        int cc_uid = cc->root_instruction()->unique_id();
+        user_list_offsets[uid_to_node_ind_[cc_uid]]++;
+        for (auto param : cc->parameter_instructions()) {
+          int param_uid = param->unique_id();
+          // add operand count to called comp's each parameter instruction.
+          operand_list_offsets[uid_to_node_ind_[param_uid]]++;
+        }
+      }
+    }
+  }
+
+  // running offsets are used to keep track of inserting indices
+  // when creating column indices arrays.
+  std::vector<int> running_user_offset;
+  std::vector<int> running_operand_offset;
+  size_t user_offset = 0;
+  size_t operand_offset = 0;
+
+  // compute exclusive prefix sum
+  for (int i = 0; i < user_list_offsets.size(); ++i) {
+    auto inst = uid_to_inst_[node_feats.uids[i]];
+    size_t ucount = user_list_offsets[i];
+    size_t opcount = operand_list_offsets[i];
+    node_feats.num_users.push_back(ucount);
+    node_feats.num_operands.push_back(opcount);
+    user_list_offsets[i] = user_offset;
+    operand_list_offsets[i] = operand_offset;
+    running_user_offset.push_back(user_offset);
+    running_operand_offset.push_back(operand_offset);
+    user_offset += ucount;
+    operand_offset += opcount;
+  }
+  user_list_offsets.push_back(user_offset);
+  operand_list_offsets.push_back(operand_offset);
+  user_list_indices.resize(user_offset);
+  operand_list_indices.resize(operand_offset);
+
+  // prepare column indices
+  for (auto inst : inst_list) {
+    // find current node's index in offsets lists
+    // and insert node indices of its users and operands
+    // to according indices lists.
+    int uid = inst->unique_id();
+    int node_idx = uid_to_node_ind_[uid];
+    int empty_comp_ucount = 0;
+
+    // Handle cases when instruction's called computation is empty.
+    auto called_comps = inst->called_computations();
+    for (auto u : inst->users()) {
+      if (u->called_computations().empty()) {
+        user_list_indices[running_user_offset[node_idx]++] =
+            uid_to_node_ind_[u->unique_id()];
+      }
+    }
+    if (called_comps.empty()) {
+      for (auto u : inst->operands()) {
+        operand_list_indices[running_operand_offset[node_idx]++] =
+            uid_to_node_ind_[u->unique_id()];
+      }
+    }
+
+    // Handle cases when instruction with called_computations:
+    auto operands = inst->operands();
+    if (called_comps.size() > 0) {
+      for (auto cc : called_comps) {
+        auto params = cc->parameter_instructions();
+        // 1. make sure number of operands of current instruction equals to
+        // each called_computation's parameter count. (as mentioned above they
+        // should have a one-to-one mapping)
+        CHECK_EQ(params.size(), operands.size());
+
+        // 2. at each index, add comp's param as user to inst's operand
+        // 3. at each index, add inst's operand as operand to comp's param
+        for (int idx = 0; idx < params.size(); ++idx) {
+          int operand_inst_idx = uid_to_node_ind_[operands[idx]->unique_id()];
+          int param_inst_idx = uid_to_node_ind_[params[idx]->unique_id()];
+          user_list_indices[running_user_offset[operand_inst_idx]++] =
+              param_inst_idx;
+          operand_list_indices[running_operand_offset[param_inst_idx]++] =
+              operand_inst_idx;
+        }
+      }
+      // 4. for root instruction of each computation, add curent node index
+      // to their user list indices.
+      for (auto cc : called_comps) {
+        int cc_uid = cc->root_instruction()->unique_id();
+        int cc_root_inst_idx = uid_to_node_ind_[cc_uid];
+
+        // 5. for current node, add each computation's root instruction as its
+        // operand. add current instruction as computation's root instruction's
+        // user.
+        user_list_indices[running_user_offset[cc_root_inst_idx]++] = node_idx;
+        operand_list_indices[running_operand_offset[node_idx]++] =
+            cc_root_inst_idx;
+      }
+    }
+  }
+
+  // add to edge features
+  // uids, srcs, dsts, shapes, layouts, dtypes
+  auto genuid = [](int src_uid, int dst_uid) -> int64_t {
+    int64_t suid = absl::bit_cast<int>(src_uid);
+    int64_t duid = absl::bit_cast<int>(dst_uid);
+    return (suid << 32) | duid;
+  };
+  for (int i = 0; i < user_list_offsets.size() - 1; ++i) {
+    size_t user_offset = user_list_offsets[i];
+    size_t operand_offset = operand_list_offsets[i];
+    int cur_uid = node_feats.uids[i];
+    auto cur_inst = uid_to_inst_[cur_uid];
+
+    if (cur_inst->opcode() == HloOpcode::kAlternatives) {
+      alternative_indices_.push_back(i);
+    }
+    for (int s = user_offset; s < user_list_offsets[i + 1]; ++s) {
+      int user_node_idx = user_list_indices[s];
+      int user_uid = node_feats.uids[user_node_idx];
+      auto user_inst = uid_to_inst_[user_uid];
+
+      // add to out edge features
+      int64_t euid = genuid(cur_uid, user_uid);
+      out_edge_feats.uids.push_back(euid);
+      out_edge_feats.srcs.push_back(i);
+      out_edge_feats.dsts.push_back(user_node_idx);
+      // put in shapes, layouts, and dtypes for cur_inst
+      Shape shape = cur_inst->shape();
+      auto minor_to_major = shape.layout().minor_to_major();
+      int dim_size = shape.dimensions_size();
+      for (int k = 0; k < 8; ++k) {
+        if (k < dim_size) {
+          out_edge_feats.dims.push_back(shape.dimensions(k));
+          out_edge_feats.layouts.push_back(minor_to_major[k]);
+        } else {
+          out_edge_feats.dims.push_back(-1);
+          out_edge_feats.layouts.push_back(-1);
+        }
+      }
+      out_edge_feats.dtypes.push_back(shape.element_type());
+    }
+    for (int s = operand_offset; s < operand_list_offsets[i + 1]; ++s) {
+      int operand_node_idx = operand_list_indices[s];
+      int operand_uid = node_feats.uids[operand_node_idx];
+      auto operand_inst = uid_to_inst_[operand_uid];
+
+      // add to out edge features
+      int64_t euid = genuid(operand_uid, cur_uid);
+      in_edge_feats.uids.push_back(euid);
+      in_edge_feats.srcs.push_back(operand_node_idx);
+      in_edge_feats.dsts.push_back(i);
+      // put in shapes, layouts, and dtypes for operand_inst
+      Shape shape = operand_inst->shape();
+      auto minor_to_major = shape.layout().minor_to_major();
+      int dim_size = shape.dimensions_size();
+      for (int k = 0; k < 8; ++k) {
+        if (k < dim_size) {
+          in_edge_feats.dims.push_back(shape.dimensions(k));
+          in_edge_feats.layouts.push_back(minor_to_major[k]);
+        } else {
+          in_edge_feats.dims.push_back(-1);
+          in_edge_feats.layouts.push_back(-1);
+        }
+      }
+      in_edge_feats.dtypes.push_back(shape.element_type());
+    }
+  }
+
+  // final touch, add the root_index
+  int entry_root_uid = m->entry_computation()->root_instruction()->unique_id();
+  root_index_ = uid_to_node_ind_[entry_root_uid];
+
+  LOG(ERROR) << "HloGraph build finished";
+
+  return true;
+}
+
+void HloGraph::ShowStats() {
+  auto oedge_offsets = get_out_edge_offsets();
+  auto iedge_offsets = get_in_edge_offsets();
+  auto oedge_indices = get_out_edge_indices();
+  auto iedge_indices = get_in_edge_indices();
+  LOG(ERROR) << "module name: " << name_;
+  LOG(ERROR) << "number of nodes: " << oedge_offsets.size() - 1;
+  LOG(ERROR) << "number of in edges: " << iedge_offsets.back();
+  LOG(ERROR) << "number of out edges: " << oedge_offsets.back();
+  LOG(ERROR) << "================================";
+
+  auto print_vector = [](int64_t* vec) -> std::string {
+    std::string os;
+    os = "[";
+    for (int i = 0; i < 8; ++i) {
+      os += std::to_string(vec[i]);
+      os += ",";
+    }
+    os += "]";
+    return os;
+  };
+
+  auto names = get_node_names();
+  auto gids = get_gids();
+  auto ucounts = get_user_counts();
+  auto opcounts = get_operand_counts();
+
+  auto oedge_uids = get_out_edge_uids();
+  auto oedge_srcs = get_out_edge_srcs();
+  auto oedge_dsts = get_out_edge_dsts();
+  auto oedge_dims = get_out_edge_dims();
+  auto oedge_layouts = get_out_edge_layouts();
+  auto oedge_dtypes = get_out_edge_dtypes();
+
+  auto iedge_uids = get_in_edge_uids();
+  auto iedge_srcs = get_in_edge_srcs();
+  auto iedge_dsts = get_in_edge_dsts();
+  auto iedge_dims = get_in_edge_dims();
+  auto iedge_layouts = get_in_edge_layouts();
+  auto iedge_dtypes = get_in_edge_dtypes();
+
+  for (int i = 0; i < oedge_offsets.size() - 1; ++i) {
+    LOG(ERROR) << "node index: " << i;
+    LOG(ERROR) << "node name: " << names[i];
+    LOG(ERROR) << "gid: " << gids[i];
+    LOG(ERROR) << "user_count: " << ucounts[i];
+    LOG(ERROR) << "operand_count: " << opcounts[i];
+    int start_idx = oedge_offsets[i];
+    int end_idx = oedge_offsets[i + 1];
+    for (int ii = start_idx; ii < end_idx; ++ii) {
+      int idx = oedge_indices[ii];
+      LOG(ERROR) << "  out edge: " << idx << " " << names[idx];
+      LOG(ERROR) << "  " << oedge_uids[ii] << " | " << oedge_srcs[ii] << "->"
+                 << oedge_dsts[ii];
+      LOG(ERROR) << "  dims: " << print_vector(&oedge_dims[ii * 8]);
+      LOG(ERROR) << "  layouts: " << print_vector(&oedge_layouts[ii * 8]);
+      LOG(ERROR) << "  dtype: " << oedge_dtypes[ii];
+    }
+    start_idx = iedge_offsets[i];
+    end_idx = iedge_offsets[i + 1];
+    for (int ii = start_idx; ii < end_idx; ++ii) {
+      int idx = iedge_indices[ii];
+      LOG(ERROR) << "  in edge: " << idx << " " << names[idx];
+      LOG(ERROR) << "  " << iedge_uids[ii] << " | " << iedge_srcs[ii] << "->"
+                 << iedge_dsts[ii];
+      LOG(ERROR) << "  dims: " << print_vector(&iedge_dims[ii * 8]);
+      LOG(ERROR) << "  layouts: " << print_vector(&iedge_layouts[ii * 8]);
+      LOG(ERROR) << "  dtype: " << iedge_dtypes[ii];
+    }
+    LOG(ERROR) << "--------------------------------";
+  }
+}
+
+}  // namespace xla
diff --git a/tensorflow/xla_standalone/hlo_graph.h b/tensorflow/xla_standalone/hlo_graph.h
new file mode 100644
index 00000000000..8e491dd2720
--- /dev/null
+++ b/tensorflow/xla_standalone/hlo_graph.h
@@ -0,0 +1,202 @@
+// Copyright 2022 Garena Online Private Limited.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+#ifndef TENSORFLOW_XLA_STANDALONE_HLO_GRAPH_H_
+#define TENSORFLOW_XLA_STANDALONE_HLO_GRAPH_H_
+
+#include <cstdio>
+#include <iostream>
+#include <string>
+#include <tuple>
+#include <vector>
+
+#include "absl/container/flat_hash_map.h"
+#include "absl/container/inlined_vector.h"
+#include "tensorflow/compiler/xla/service/hlo_computation.h"
+#include "tensorflow/compiler/xla/service/hlo_instruction.h"
+#include "tensorflow/compiler/xla/service/hlo_module.h"
+
+namespace xla {
+
+// NodeFeatures holds all tensors with #node length
+// uids: unique_id of HloInstruction
+// names: opcode in string
+// gids: sub-computation id HloInstruction belongs to
+// num_users: number of outgoing edges (users and called computations)
+// num_operands: number of incoming edges (operands)
+struct NodeFeats {
+  std::vector<int> uids;
+  std::vector<std::string> names;
+  std::vector<size_t> gids;
+  std::vector<int> num_users;
+  std::vector<int> num_operands;
+
+  void Clear() {
+    uids.clear();
+    names.clear();
+    gids.clear();
+    num_users.clear();
+    num_operands.clear();
+  }
+};
+
+// EdgeFeatures holds all tensors with #edge length
+// uids: unique_id of both src and dst node.
+// srcs: indices of source nodes
+// dsts: indices of destination nodes
+// dims: a fixed-length (8) array to present tensor shape
+// layouts: a fixed-length (8) array to present tensor layout
+// dtypes: tensor dtype
+// enum PrimitiveType {
+//   S16 = 0,
+//   S32,
+//   S64,
+//   U8,
+//   U16,
+//   U32,
+//   U64,
+//   F16,
+//   BF16,
+//   F32,
+//   F64,
+//   C64,
+//   C128
+// };
+struct EdgeFeats {
+  std::vector<int64_t> uids;
+  std::vector<int> srcs;
+  std::vector<int> dsts;
+  std::vector<int64_t> dims;
+  std::vector<int64_t> layouts;
+  // PrimitiveType as is defined in xla_data.proto.
+  std::vector<PrimitiveType> dtypes;
+
+  void Clear() {
+    uids.clear();
+    srcs.clear();
+    dsts.clear();
+    dims.clear();
+    layouts.clear();
+    dtypes.clear();
+  }
+};
+
+// A class for representing a HLO graph in the module
+//
+// To make things simpler, only string, f32, i32, and i64 are allowed as dtype.
+//
+class HloGraph {
+ public:
+  HloGraph() {}
+  explicit HloGraph(const HloModule* m);
+
+  bool Build(const HloModule* m);
+
+  void Clear();
+
+  void ShowStats();
+
+  // return CSR/CSC
+  std::vector<size_t>& get_out_edge_offsets() { return user_list_offsets; }
+  std::vector<size_t>& get_out_edge_indices() { return user_list_indices; }
+  std::vector<size_t>& get_in_edge_offsets() { return operand_list_offsets; }
+  std::vector<size_t>& get_in_edge_indices() { return operand_list_indices; }
+
+  // return node features.
+  const std::vector<int>& get_node_uids() { return node_feats.uids; }
+  const std::vector<std::string>& get_node_names() { return node_feats.names; }
+  const std::vector<size_t>& get_gids() { return node_feats.gids; }
+  const std::vector<int>& get_user_counts() { return node_feats.num_users; }
+  const std::vector<int>& get_operand_counts() {
+    return node_feats.num_operands;
+  }
+
+  // return edge features.
+  const std::vector<int64_t>& get_in_edge_uids() { return in_edge_feats.uids; }
+  const std::vector<int>& get_in_edge_srcs() { return in_edge_feats.srcs; }
+  const std::vector<int>& get_in_edge_dsts() { return in_edge_feats.dsts; }
+  const std::vector<int64_t>& get_in_edge_dims() { return in_edge_feats.dims; }
+  const std::vector<int64_t>& get_in_edge_layouts() {
+    return in_edge_feats.layouts;
+  }
+  const std::vector<PrimitiveType>& get_in_edge_dtypes() {
+    return in_edge_feats.dtypes;
+  }
+
+  const std::vector<int64_t>& get_out_edge_uids() {
+    return out_edge_feats.uids;
+  }
+  const std::vector<int>& get_out_edge_srcs() { return out_edge_feats.srcs; }
+  const std::vector<int>& get_out_edge_dsts() { return out_edge_feats.dsts; }
+  const std::vector<int64_t>& get_out_edge_dims() {
+    return out_edge_feats.dims;
+  }
+  const std::vector<int64_t>& get_out_edge_layouts() {
+    return out_edge_feats.layouts;
+  }
+  const std::vector<PrimitiveType>& get_out_edge_dtypes() {
+    return out_edge_feats.dtypes;
+  }
+
+  NodeFeats& get_node_feats() { return node_feats; }
+  EdgeFeats& get_in_edge_feats() { return in_edge_feats; }
+  EdgeFeats& get_out_edge_feats() { return out_edge_feats; }
+
+  std::vector<int>& get_alternative_indices() { return alternative_indices_; }
+  absl::flat_hash_map<int, HloInstruction*>& get_uid_to_inst() {
+    return uid_to_inst_;
+  }
+
+  // TODO(wangyzh): add more utility functions
+
+  // something I can think of now:
+  // get neighbors (up to some layers with samples)
+  // see if one edge is cross computation
+  // a lot of others we need to decide whether to do them in python
+  // or here. Since a lot of them are filters with different
+  // pre-conditions.
+
+ private:
+  HloModule* parent_hlo_module_;
+  int uid_;
+  std::string name_;
+  // Use CSR to represent graph (and CSC inverse graph) topology
+  std::vector<size_t> user_list_offsets;
+  std::vector<size_t> user_list_indices;
+  std::vector<size_t> operand_list_offsets;
+  std::vector<size_t> operand_list_indices;
+
+  // Ignore control deps for now
+  // utility to lookup node and its neighbor
+  absl::flat_hash_map<int, int> uid_to_node_ind_;
+  absl::flat_hash_map<int, HloInstruction*> uid_to_inst_;
+
+ private:
+  // Indices of alternative nodes
+  std::vector<int> alternative_indices_;
+
+  // index of root instruction of entry computation
+  int root_index_;
+
+  // Node features
+  NodeFeats node_feats;
+
+  // Edge features
+  EdgeFeats in_edge_feats;
+  EdgeFeats out_edge_feats;
+};
+
+}  // namespace xla
+
+#endif  // TENSORFLOW_XLA_STANDALONE_HLO_GRAPH_H_
diff --git a/tensorflow/xla_standalone/hlo_test.txt b/tensorflow/xla_standalone/hlo_test.txt
new file mode 100644
index 00000000000..ecfbc31eada
--- /dev/null
+++ b/tensorflow/xla_standalone/hlo_test.txt
@@ -0,0 +1,169 @@
+HloModule jit_train_step.151
+
+%primitive_computation_add__6.23 (parameter.24: f32[], parameter.25: f32[]) -> f32[] {
+  %constant.26 = pred[] constant(false)
+  %parameter.24 = f32[] parameter(0)
+  %parameter.25 = f32[] parameter(1)
+  ROOT %add.27 = f32[] add(f32[] %parameter.24, f32[] %parameter.25), metadata={op_type="add" op_name="add" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+}
+
+%primitive_computation_add__7.36 (parameter.37: f32[], parameter.38: f32[]) -> f32[] {
+  %constant.39 = pred[] constant(false)
+  %parameter.37 = f32[] parameter(0)
+  %parameter.38 = f32[] parameter(1)
+  ROOT %add.40 = f32[] add(f32[] %parameter.37, f32[] %parameter.38), metadata={op_type="add" op_name="add" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+}
+
+%primitive_computation_add__8.48 (parameter.49: f32[], parameter.50: f32[]) -> f32[] {
+  %constant.51 = pred[] constant(false)
+  %parameter.49 = f32[] parameter(0)
+  %parameter.50 = f32[] parameter(1)
+  ROOT %add.52 = f32[] add(f32[] %parameter.49, f32[] %parameter.50), metadata={op_type="add" op_name="add" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+}
+
+%primitive_computation_add__9.60 (parameter.61: f32[], parameter.62: f32[]) -> f32[] {
+  %constant.63 = pred[] constant(false)
+  %parameter.61 = f32[] parameter(0)
+  %parameter.62 = f32[] parameter(1)
+  ROOT %add.64 = f32[] add(f32[] %parameter.61, f32[] %parameter.62), metadata={op_type="add" op_name="add" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+}
+
+%primitive_computation_add__10.75 (parameter.76: f32[], parameter.77: f32[]) -> f32[] {
+  %constant.78 = pred[] constant(false)
+  %parameter.76 = f32[] parameter(0)
+  %parameter.77 = f32[] parameter(1)
+  ROOT %add.79 = f32[] add(f32[] %parameter.76, f32[] %parameter.77), metadata={op_type="add" op_name="add" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+}
+
+%primitive_computation_add__11.108 (parameter.109: f32[], parameter.110: f32[]) -> f32[] {
+  %constant.111 = pred[] constant(false)
+  %parameter.109 = f32[] parameter(0)
+  %parameter.110 = f32[] parameter(1)
+  ROOT %add.112 = f32[] add(f32[] %parameter.109, f32[] %parameter.110), metadata={op_type="add" op_name="add" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=184}
+}
+
+%primitive_computation_add__12.124 (parameter.125: f32[], parameter.126: f32[]) -> f32[] {
+  %constant.127 = pred[] constant(false)
+  %parameter.125 = f32[] parameter(0)
+  %parameter.126 = f32[] parameter(1)
+  ROOT %add.128 = f32[] add(f32[] %parameter.125, f32[] %parameter.126), metadata={op_type="add" op_name="add" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=184}
+}
+
+ENTRY %jit_train_step.151 (parameter.1: f32[128], parameter.2: f32[1,128], parameter.3: f32[1], parameter.4: f32[128,1], parameter.5: f32[200,1], parameter.6: f32[200,1]) -> (f32[128], f32[1,128], f32[1], f32[128,1]) {
+  %constant.7 = pred[] constant(false)
+  %parameter.5 = f32[200,1]{1,0} parameter(4)
+  %parameter.2 = f32[1,128]{1,0} parameter(1)
+  %dot.8 = f32[200,128]{1,0} dot(f32[200,1]{1,0} %parameter.5, f32[1,128]{1,0} %parameter.2), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="dot_general" op_name="jit(train_step)/dot_general[\n  dimension_numbers=(((1,), (0,)), ((), ()))\n  precision=None\n  preferred_element_type=None\n]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=180}
+  %parameter.1 = f32[128]{0} parameter(0)
+  %broadcast.9 = f32[200,128]{1,0} broadcast(f32[128]{0} %parameter.1), dimensions={1}, metadata={op_type="broadcast_in_dim" op_name="jit(train_step)/broadcast_in_dim[broadcast_dimensions=(1,) shape=(200, 128)]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=184}
+  %add.10 = f32[200,128]{1,0} add(f32[200,128]{1,0} %dot.8, f32[200,128]{1,0} %broadcast.9), metadata={op_type="add" op_name="jit(train_step)/add" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=185}
+  %tanh.11 = f32[200,128]{1,0} tanh(f32[200,128]{1,0} %add.10), metadata={op_type="tanh" op_name="jit(train_step)/tanh" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=127}
+  %parameter.4 = f32[128,1]{1,0} parameter(3)
+  %dot.15 = f32[200,1]{1,0} dot(f32[200,128]{1,0} %tanh.11, f32[128,1]{1,0} %parameter.4), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="dot_general" op_name="jit(train_step)/dot_general[\n  dimension_numbers=(((1,), (0,)), ((), ()))\n  precision=None\n  preferred_element_type=None\n]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=180}
+  %parameter.3 = f32[1]{0} parameter(2)
+  %broadcast.16 = f32[200,1]{1,0} broadcast(f32[1]{0} %parameter.3), dimensions={1}, metadata={op_type="broadcast_in_dim" op_name="jit(train_step)/broadcast_in_dim[broadcast_dimensions=(1,) shape=(200, 1)]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=184}
+  %add.17 = f32[200,1]{1,0} add(f32[200,1]{1,0} %dot.15, f32[200,1]{1,0} %broadcast.16), metadata={op_type="add" op_name="jit(train_step)/add" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=185}
+  %parameter.6 = f32[200,1]{1,0} parameter(5)
+  %subtract.69 = f32[200,1]{1,0} subtract(f32[200,1]{1,0} %add.17, f32[200,1]{1,0} %parameter.6), metadata={op_type="sub" op_name="jit(train_step)/sub" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %multiply.70 = f32[200,1]{1,0} multiply(f32[200,1]{1,0} %subtract.69, f32[200,1]{1,0} %subtract.69), metadata={op_type="integer_pow" op_name="jit(train_step)/integer_pow[y=2]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %constant.74 = f32[] constant(0), metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0, 1)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %reduce.80 = f32[] reduce(f32[200,1]{1,0} %multiply.70, f32[] %constant.74), dimensions={0,1}, to_apply=%primitive_computation_add__10.75, metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0, 1)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %constant.81 = f32[] constant(200), metadata={op_type="div" op_name="jit(train_step)/div" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %divide.82 = f32[] divide(f32[] %reduce.80, f32[] %constant.81), metadata={op_type="div" op_name="jit(train_step)/div" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %negate.83 = f32[] negate(f32[] %divide.82), metadata={op_type="neg" op_name="jit(train_step)/neg" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %constant.84 = f32[] constant(0.5), metadata={op_type="div" op_name="jit(train_step)/div" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %divide.85 = f32[] divide(f32[] %negate.83, f32[] %constant.84), metadata={op_type="div" op_name="jit(train_step)/div" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %multiply.18 = f32[128]{0} multiply(f32[128]{0} %parameter.1, f32[128]{0} %parameter.1), metadata={op_type="integer_pow" op_name="jit(train_step)/integer_pow[y=2]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %constant.22 = f32[] constant(0), metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0,)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %reduce.28 = f32[] reduce(f32[128]{0} %multiply.18, f32[] %constant.22), dimensions={0}, to_apply=%primitive_computation_add__6.23, metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0,)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %constant.29 = f32[] constant(0), metadata={op_type="add" op_name="jit(train_step)/add" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %add.30 = f32[] add(f32[] %reduce.28, f32[] %constant.29), metadata={op_type="add" op_name="jit(train_step)/add" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.31 = f32[1,128]{1,0} multiply(f32[1,128]{1,0} %parameter.2, f32[1,128]{1,0} %parameter.2), metadata={op_type="integer_pow" op_name="jit(train_step)/integer_pow[y=2]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %constant.35 = f32[] constant(0), metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0, 1)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %reduce.41 = f32[] reduce(f32[1,128]{1,0} %multiply.31, f32[] %constant.35), dimensions={0,1}, to_apply=%primitive_computation_add__7.36, metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0, 1)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %add.42 = f32[] add(f32[] %add.30, f32[] %reduce.41), metadata={op_type="add" op_name="jit(train_step)/add" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.43 = f32[1]{0} multiply(f32[1]{0} %parameter.3, f32[1]{0} %parameter.3), metadata={op_type="integer_pow" op_name="jit(train_step)/integer_pow[y=2]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %constant.47 = f32[] constant(0), metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0,)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %reduce.53 = f32[] reduce(f32[1]{0} %multiply.43, f32[] %constant.47), dimensions={0}, to_apply=%primitive_computation_add__8.48, metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0,)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %add.54 = f32[] add(f32[] %add.42, f32[] %reduce.53), metadata={op_type="add" op_name="jit(train_step)/add" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.55 = f32[128,1]{1,0} multiply(f32[128,1]{1,0} %parameter.4, f32[128,1]{1,0} %parameter.4), metadata={op_type="integer_pow" op_name="jit(train_step)/integer_pow[y=2]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %constant.59 = f32[] constant(0), metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0, 1)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %reduce.65 = f32[] reduce(f32[128,1]{1,0} %multiply.55, f32[] %constant.59), dimensions={0,1}, to_apply=%primitive_computation_add__9.60, metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0, 1)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %add.66 = f32[] add(f32[] %add.54, f32[] %reduce.65), metadata={op_type="add" op_name="jit(train_step)/add" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %constant.67 = f32[] constant(-0), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.68 = f32[] multiply(f32[] %add.66, f32[] %constant.67), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %add.86 = f32[] add(f32[] %divide.85, f32[] %multiply.68), metadata={op_type="add" op_name="jit(train_step)/add" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=63}
+  %negate.87 = f32[] negate(f32[] %add.86), metadata={op_type="neg" op_name="jit(train_step)/neg" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=66}
+  %constant.88 = f32[] constant(1), metadata={op_type="neg" op_name="jit(train_step)/neg" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=66}
+  %negate.89 = f32[] negate(f32[] %constant.88), metadata={op_type="neg" op_name="jit(train_step)/neg" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=66}
+  %constant.90 = f32[] constant(-0), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.91 = f32[] multiply(f32[] %negate.89, f32[] %constant.90), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %broadcast.98 = f32[128]{0} broadcast(f32[] %multiply.91), dimensions={}, metadata={op_type="broadcast_in_dim" op_name="jit(train_step)/broadcast_in_dim[broadcast_dimensions=() shape=(128,)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %constant.19 = f32[] constant(2), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %broadcast.20 = f32[128]{0} broadcast(f32[] %constant.19), dimensions={}, metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.21 = f32[128]{0} multiply(f32[128]{0} %broadcast.20, f32[128]{0} %parameter.1), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.99 = f32[128]{0} multiply(f32[128]{0} %broadcast.98, f32[128]{0} %multiply.21), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %constant.100 = f32[] constant(0.5), metadata={op_type="div" op_name="jit(train_step)/div" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %divide.101 = f32[] divide(f32[] %negate.89, f32[] %constant.100), metadata={op_type="div" op_name="jit(train_step)/div" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %negate.102 = f32[] negate(f32[] %divide.101), metadata={op_type="neg" op_name="jit(train_step)/neg" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %constant.103 = f32[] constant(200), metadata={op_type="div" op_name="jit(train_step)/div" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %divide.104 = f32[] divide(f32[] %negate.102, f32[] %constant.103), metadata={op_type="div" op_name="jit(train_step)/div" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %broadcast.105 = f32[200,1]{1,0} broadcast(f32[] %divide.104), dimensions={}, metadata={op_type="broadcast_in_dim" op_name="jit(train_step)/broadcast_in_dim[broadcast_dimensions=() shape=(200, 1)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %constant.71 = f32[] constant(2), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %broadcast.72 = f32[200,1]{1,0} broadcast(f32[] %constant.71), dimensions={}, metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %multiply.73 = f32[200,1]{1,0} multiply(f32[200,1]{1,0} %broadcast.72, f32[200,1]{1,0} %subtract.69), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %multiply.106 = f32[200,1]{1,0} multiply(f32[200,1]{1,0} %broadcast.105, f32[200,1]{1,0} %multiply.73), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=62}
+  %dot.118 = f32[200,128]{1,0} dot(f32[200,1]{1,0} %multiply.106, f32[128,1]{1,0} %parameter.4), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_type="dot_general" op_name="jit(train_step)/dot_general[\n  dimension_numbers=(((1,), (1,)), ((), ()))\n  precision=None\n  preferred_element_type=None\n]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=180}
+  %constant.12 = f32[] constant(1), metadata={op_type="sub" op_name="jit(train_step)/sub" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=127}
+  %broadcast.13 = f32[200,128]{1,0} broadcast(f32[] %constant.12), dimensions={}, metadata={op_type="sub" op_name="jit(train_step)/sub" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=127}
+  %subtract.14 = f32[200,128]{1,0} subtract(f32[200,128]{1,0} %broadcast.13, f32[200,128]{1,0} %tanh.11), metadata={op_type="sub" op_name="jit(train_step)/sub" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=127}
+  %multiply.120 = f32[200,128]{1,0} multiply(f32[200,128]{1,0} %dot.118, f32[200,128]{1,0} %subtract.14), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=127}
+  %multiply.121 = f32[200,128]{1,0} multiply(f32[200,128]{1,0} %multiply.120, f32[200,128]{1,0} %tanh.11), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=127}
+  %add.122 = f32[200,128]{1,0} add(f32[200,128]{1,0} %multiply.120, f32[200,128]{1,0} %multiply.121), metadata={op_type="add_any" op_name="jit(train_step)/add_any" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=127}
+  %constant.123 = f32[] constant(0), metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0,)]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=184}
+  %reduce.129 = f32[128]{0} reduce(f32[200,128]{1,0} %add.122, f32[] %constant.123), dimensions={0}, to_apply=%primitive_computation_add__12.124, metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0,)]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=184}
+  %add.130 = f32[128]{0} add(f32[128]{0} %multiply.99, f32[128]{0} %reduce.129), metadata={op_type="add_any" op_name="jit(train_step)/add_any" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=71}
+  %constant.134 = f32[] constant(0.001), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %broadcast.135 = f32[128]{0} broadcast(f32[] %constant.134), dimensions={}, metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %multiply.136 = f32[128]{0} multiply(f32[128]{0} %add.130, f32[128]{0} %broadcast.135), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %subtract.137 = f32[128]{0} subtract(f32[128]{0} %parameter.1, f32[128]{0} %multiply.136), metadata={op_type="sub" op_name="jit(train_step)/sub" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %broadcast.96 = f32[1,128]{1,0} broadcast(f32[] %multiply.91), dimensions={}, metadata={op_type="broadcast_in_dim" op_name="jit(train_step)/broadcast_in_dim[broadcast_dimensions=() shape=(1, 128)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %constant.32 = f32[] constant(2), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %broadcast.33 = f32[1,128]{1,0} broadcast(f32[] %constant.32), dimensions={}, metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.34 = f32[1,128]{1,0} multiply(f32[1,128]{1,0} %broadcast.33, f32[1,128]{1,0} %parameter.2), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.97 = f32[1,128]{1,0} multiply(f32[1,128]{1,0} %broadcast.96, f32[1,128]{1,0} %multiply.34), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %dot.131 = f32[128,1]{1,0} dot(f32[200,128]{1,0} %add.122, f32[200,1]{1,0} %parameter.5), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_type="dot_general" op_name="jit(train_step)/dot_general[\n  dimension_numbers=(((0,), (0,)), ((), ()))\n  precision=None\n  preferred_element_type=None\n]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=180}
+  %transpose.132 = f32[1,128]{0,1} transpose(f32[128,1]{1,0} %dot.131), dimensions={1,0}, metadata={op_type="transpose" op_name="jit(train_step)/transpose[permutation=(1, 0)]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=180}
+  %add.133 = f32[1,128]{1,0} add(f32[1,128]{1,0} %multiply.97, f32[1,128]{0,1} %transpose.132), metadata={op_type="add_any" op_name="jit(train_step)/add_any" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=71}
+  %constant.138 = f32[] constant(0.001), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %broadcast.139 = f32[1,128]{1,0} broadcast(f32[] %constant.138), dimensions={}, metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %multiply.140 = f32[1,128]{1,0} multiply(f32[1,128]{1,0} %add.133, f32[1,128]{1,0} %broadcast.139), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %subtract.141 = f32[1,128]{1,0} subtract(f32[1,128]{1,0} %parameter.2, f32[1,128]{1,0} %multiply.140), metadata={op_type="sub" op_name="jit(train_step)/sub" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %broadcast.94 = f32[1]{0} broadcast(f32[] %multiply.91), dimensions={}, metadata={op_type="broadcast_in_dim" op_name="jit(train_step)/broadcast_in_dim[broadcast_dimensions=() shape=(1,)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %constant.44 = f32[] constant(2), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %broadcast.45 = f32[1]{0} broadcast(f32[] %constant.44), dimensions={}, metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.46 = f32[1]{0} multiply(f32[1]{0} %broadcast.45, f32[1]{0} %parameter.3), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.95 = f32[1]{0} multiply(f32[1]{0} %broadcast.94, f32[1]{0} %multiply.46), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %constant.107 = f32[] constant(0), metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0, 1)]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=184}
+  %reduce.113 = f32[] reduce(f32[200,1]{1,0} %multiply.106, f32[] %constant.107), dimensions={0,1}, to_apply=%primitive_computation_add__11.108, metadata={op_type="reduce_sum" op_name="jit(train_step)/reduce_sum[axes=(0, 1)]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=184}
+  %broadcast.114 = f32[1]{0} broadcast(f32[] %reduce.113), dimensions={}, metadata={op_type="broadcast_in_dim" op_name="jit(train_step)/broadcast_in_dim[broadcast_dimensions=() shape=(1,)]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=184}
+  %add.115 = f32[1]{0} add(f32[1]{0} %multiply.95, f32[1]{0} %broadcast.114), metadata={op_type="add_any" op_name="jit(train_step)/add_any" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=71}
+  %constant.142 = f32[] constant(0.001), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %broadcast.143 = f32[1]{0} broadcast(f32[] %constant.142), dimensions={}, metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %multiply.144 = f32[1]{0} multiply(f32[1]{0} %add.115, f32[1]{0} %broadcast.143), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %subtract.145 = f32[1]{0} subtract(f32[1]{0} %parameter.3, f32[1]{0} %multiply.144), metadata={op_type="sub" op_name="jit(train_step)/sub" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %broadcast.92 = f32[128,1]{1,0} broadcast(f32[] %multiply.91), dimensions={}, metadata={op_type="broadcast_in_dim" op_name="jit(train_step)/broadcast_in_dim[broadcast_dimensions=() shape=(128, 1)]" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %constant.56 = f32[] constant(2), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %broadcast.57 = f32[128,1]{1,0} broadcast(f32[] %constant.56), dimensions={}, metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.58 = f32[128,1]{1,0} multiply(f32[128,1]{1,0} %broadcast.57, f32[128,1]{1,0} %parameter.4), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %multiply.93 = f32[128,1]{1,0} multiply(f32[128,1]{1,0} %broadcast.92, f32[128,1]{1,0} %multiply.58), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=60}
+  %dot.116 = f32[1,128]{1,0} dot(f32[200,1]{1,0} %multiply.106, f32[200,128]{1,0} %tanh.11), lhs_contracting_dims={0}, rhs_contracting_dims={0}, metadata={op_type="dot_general" op_name="jit(train_step)/dot_general[\n  dimension_numbers=(((0,), (0,)), ((), ()))\n  precision=None\n  preferred_element_type=None\n]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=180}
+  %transpose.117 = f32[128,1]{0,1} transpose(f32[1,128]{1,0} %dot.116), dimensions={1,0}, metadata={op_type="transpose" op_name="jit(train_step)/transpose[permutation=(1, 0)]" source_file="/mnt/home/linmin/g/lib/python3.8/site-packages/haiku/_src/basic.py" source_line=180}
+  %add.119 = f32[128,1]{1,0} add(f32[128,1]{1,0} %multiply.93, f32[128,1]{0,1} %transpose.117), metadata={op_type="add_any" op_name="jit(train_step)/add_any" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=71}
+  %constant.146 = f32[] constant(0.001), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %broadcast.147 = f32[128,1]{1,0} broadcast(f32[] %constant.146), dimensions={}, metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %multiply.148 = f32[128,1]{1,0} multiply(f32[128,1]{1,0} %add.119, f32[128,1]{1,0} %broadcast.147), metadata={op_type="mul" op_name="jit(train_step)/mul" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  %subtract.149 = f32[128,1]{1,0} subtract(f32[128,1]{1,0} %parameter.4, f32[128,1]{1,0} %multiply.148), metadata={op_type="sub" op_name="jit(train_step)/sub" source_file="examples/deep/nn_regression/mlp_regression.py" source_line=72}
+  ROOT %tuple.150 = (f32[128]{0}, f32[1,128]{1,0}, f32[1]{0}, f32[128,1]{1,0}) tuple(f32[128]{0} %subtract.137, f32[1,128]{1,0} %subtract.141, f32[1]{0} %subtract.145, f32[128,1]{1,0} %subtract.149)
+}
+
diff --git a/tensorflow/xla_standalone/xla_compile.cc b/tensorflow/xla_standalone/xla_compile.cc
new file mode 100644
index 00000000000..2e5d52a28aa
--- /dev/null
+++ b/tensorflow/xla_standalone/xla_compile.cc
@@ -0,0 +1,117 @@
+// Copyright 2022 Garena Online Private Limited.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//
+// An example for reading a HloModule from a HloProto file and execute the
+// module on PJRT CPU client.
+
+#include <gflags/gflags.h>
+
+#include <iostream>
+#include <memory>
+#include <sstream>
+#include <string>
+#include <vector>
+
+#include "tensorflow/compiler/xla/literal.h"
+#include "tensorflow/compiler/xla/literal_util.h"
+#include "tensorflow/compiler/xla/pjrt/cpu_device.h"
+#include "tensorflow/compiler/xla/pjrt/gpu_device.h"
+#include "tensorflow/compiler/xla/pjrt/pjrt_client.h"
+#include "tensorflow/compiler/xla/service/cpu/cpu_compiler.h"
+#include "tensorflow/compiler/xla/service/gpu/gpu_compiler.h"
+#include "tensorflow/compiler/xla/service/hlo_pass_pipeline.h"
+#include "tensorflow/compiler/xla/status.h"
+#include "tensorflow/compiler/xla/statusor.h"
+#include "tensorflow/compiler/xla/tools/hlo_module_loader.h"
+#include "tensorflow/core/platform/init_main.h"
+#include "tensorflow/core/platform/logging.h"
+#include "tensorflow/xla_standalone/hlo_graph.h"
+
+DEFINE_string(hlo, "-", "hlo text file");  // by default read from stdin
+DEFINE_string(dry, "", "which pass to dry run");
+DEFINE_string(platform, "gpu", "gpu or cpu, defaults to gpu");
+
+int main(int argc, char** argv) {
+  tensorflow::port::InitMain("", &argc, &argv);
+  gflags::ParseCommandLineFlags(&argc, &argv, true);
+
+  setenv("DRY", FLAGS_dry.c_str(), 1);
+  xla::HloPassPipeline::dry_sandwich_set =
+      xla::HloPassPipeline::ExtractDrySandwichSetFromEnv();
+
+  xla::Intercept<xla::cpu::CpuCompiler> cpu_intercept;
+  xla::Intercept<xla::gpu::GpuCompiler> gpu_intercept;
+
+  std::function<void(xla::HloModuleConfig*)> config_modifier_hook =
+      [](xla::HloModuleConfig* config) { config->set_seed(42); };
+
+  // Load HloModule from file.
+  std::unique_ptr<xla::HloModule> hlo_module;
+  if (FLAGS_hlo == "-") {
+    std::stringstream ss;
+    std::string s;
+    while (std::getline(std::cin, s)) {
+      ss << s << "\n";
+    }
+    hlo_module = LoadModuleFromData(ss.str(), "txt",
+                                    xla::hlo_module_loader_details::Config(),
+                                    config_modifier_hook)
+                     .ValueOrDie();
+  } else {
+    hlo_module =
+        LoadModuleFromFile(FLAGS_hlo, xla::hlo_module_loader_details::Config(),
+                           "txt", config_modifier_hook)
+            .ValueOrDie();
+  }
+  const xla::HloModuleProto hlo_module_proto = hlo_module->ToProto();
+
+  // Run it using JAX C++ Runtime (PJRT).
+  // Get a CPU client.
+
+  std::unique_ptr<xla::PjRtClient> client;
+  if (FLAGS_platform == "gpu") {
+    client = xla::GetGpuClient(/*asynchronous=*/true, xla::GpuAllocatorConfig(),
+                               nullptr, 0)
+                 .ValueOrDie();
+  } else if (FLAGS_platform == "cpu") {
+    client = xla::GetCpuClient(/*asynchronous=*/true).ValueOrDie();
+  } else {
+    LOG(FATAL) << "Unknown platform " << FLAGS_platform;
+  }
+
+  // Compile XlaComputation to PjRtExecutable.
+  xla::XlaComputation xla_computation(hlo_module_proto);
+  xla::CompileOptions compile_options;
+
+  try {
+    std::unique_ptr<xla::PjRtExecutable> executable =
+        client->Compile(xla_computation, compile_options).ValueOrDie();
+  } catch (xla::Intercept<xla::cpu::CpuCompiler>& e) {
+    cpu_intercept = std::move(e);
+  } catch (xla::Intercept<xla::gpu::GpuCompiler>& e) {
+    gpu_intercept = std::move(e);
+    gpu_intercept.compiler->RunHloPasses(gpu_intercept.module.get(),
+                                         gpu_intercept.stream_exec,
+                                         gpu_intercept.options);
+    xla::HloGraph graph(gpu_intercept.module.get());
+    graph.ShowStats();
+  }
+
+  // intercept.compiler->RunHloPasses(intercept.module.get(),
+  //                                intercept.stream_exec, intercept.options);
+
+  /// There's a very long chain here
+  /// pjrtclient -> local_client -> local_service -> service -> BuildExecutable
+  /// -> backend->compiler->RunHloPasses
+}
diff --git a/third_party/absl/hloenv_make_hash_deterministic.patch b/third_party/absl/hloenv_make_hash_deterministic.patch
new file mode 100644
index 00000000000..4c773ed5108
--- /dev/null
+++ b/third_party/absl/hloenv_make_hash_deterministic.patch
@@ -0,0 +1,20 @@
+diff --git a/absl/hash/internal/hash.h b/absl/hash/internal/hash.h
+index 45dfdd46..3e90c0f5 100644
+--- a/absl/hash/internal/hash.h
++++ b/absl/hash/internal/hash.h
+@@ -1133,14 +1133,7 @@ class ABSL_DLL MixingHashState : public HashStateBase<MixingHashState> {
+   // On other platforms this is still going to be non-deterministic but most
+   // probably per-build and not per-process.
+   ABSL_ATTRIBUTE_ALWAYS_INLINE static uint64_t Seed() {
+-#if (!defined(__clang__) || __clang_major__ > 11) && \
+-    !defined(__apple_build_version__)
+-    return static_cast<uint64_t>(reinterpret_cast<uintptr_t>(&kSeed));
+-#else
+-    // Workaround the absence of
+-    // https://github.com/llvm/llvm-project/commit/bc15bf66dcca76cc06fe71fca35b74dc4d521021.
+-    return static_cast<uint64_t>(reinterpret_cast<uintptr_t>(kSeed));
+-#endif
++    return 42;
+   }
+   static const void* const kSeed;
+ 
diff --git a/third_party/absl/workspace.bzl b/third_party/absl/workspace.bzl
index 39ba0e6ed6e..75b1f507937 100644
--- a/third_party/absl/workspace.bzl
+++ b/third_party/absl/workspace.bzl
@@ -1,14 +1,17 @@
 """Provides the repository macro to import absl."""
 
 load("//third_party:repo.bzl", "tf_http_archive", "tf_mirror_urls")
+load("@bazel_tools//tools/build_defs/repo:git.bzl", "new_git_repository")
 
 def repo():
     """Imports absl."""
 
     # Attention: tools parse and update these lines.
     # LINT.IfChange
-    ABSL_COMMIT = "215105818dfde3174fe799600bb0f3cae233d0bf"
-    ABSL_SHA256 = "237e2e6aec7571ae90d961d02de19f56861a7417acbbc15713b8926e39d461ed"
+    # ABSL_COMMIT = "215105818dfde3174fe799600bb0f3cae233d0bf"
+    # ABSL_SHA256 = "237e2e6aec7571ae90d961d02de19f56861a7417acbbc15713b8926e39d461ed"
+    ABSL_SHA256 = "520f61963f0807412d1d61f5f0dd706576b69413ee69959aa8f8715c49a78a00"
+    ABSL_COMMIT = "731689ffc2ad7bb95cc86b5b6160dbe7858f27a0"
     # LINT.ThenChange(//tensorflow/lite/tools/cmake/modules/abseil-cpp.cmake)
 
     SYS_DIRS = [
@@ -42,8 +45,7 @@ def repo():
         build_file = "//third_party/absl:com_google_absl.BUILD",
         system_build_file = "//third_party/absl:system.BUILD",
         system_link_files = SYS_LINKS,
-        # TODO(mihaimaruseac): Remove the patch when https://github.com/abseil/abseil-cpp/issues/326 is resolved
-        patch_file = ["//third_party/absl:com_google_absl_fix_mac_and_nvcc_build.patch"],
+        patch_file = ["//third_party/absl:hloenv_make_hash_deterministic.patch"],
         strip_prefix = "abseil-cpp-{commit}".format(commit = ABSL_COMMIT),
         urls = tf_mirror_urls("https://github.com/abseil/abseil-cpp/archive/{commit}.tar.gz".format(commit = ABSL_COMMIT)),
     )
